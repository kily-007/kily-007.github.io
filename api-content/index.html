{"posts":[{"title":"笔记-2020/06/08","content":"学习内容： GitHub注册，远程仓库创建。 Gridea博客搭建。 卜算子网页访问次数统计。 HbuildX安装，并查看Grdiea源码。 Html、CSS、Js简介。 作业： 撰写“关于”界面的博客，内容为自我介绍。 ","link":"https://kily-007.github.io/post/lv4tjP2cN/"},{"title":"Gridea添加百度统计","content":"Gridea默认仅支持谷歌统计，而国内访问谷歌网站受限，在此根据github上的资源，整合了添加百度统计的功能。 更改主题代码 在对应的主题文件目录下，本人为Gridea\\themes\\pure\\config.json，如下图所示，橘红色圈中为百度统计添加代码。重新启动Gridea界面，在主题-自定义配置最下面会出现百度统计的模块则第一步成功。 申请百度统计并配置JS代码 注册百度统计-站长版 https://tongji.baidu.com/web/welcome/login 添加博客网站到百度统计 百度统计JS代码获取 将对应的百度统计JS代码添加到Gridea界面中的主题-自定义配置-百度统计中 ","link":"https://kily-007.github.io/post/gridea-tian-jia-bai-du-tong-ji/"},{"title":"序贯相似性检测算法(SSDA)","content":"序贯相似性检测算法（SSDA） 传统的模板匹配算法的基本搜索策略是遍历性的，为了找到最优匹配点，传统方法均必须在搜索区域内的每一个像素点上进行区域相关匹配计算，图像相关匹配的数据量和计算量很大，匹配速度较慢，序贯相似性检测算法（SSDA）是针对传统模板匹配算法提出的一种高效的图像匹配算法。具体算法是先初步搜索，再精搜索，搜索的范围一步一步减小。 思路 SSDA通过人为设定一个固定阈值，及早地终止在不匹配位置上的计算，以此减小计算量，达到提高运算速度的目的。其步骤如下： （1）选取一个误差准则，作为终止不匹配点计算的标准，通常可选取绝对误差 （2）设定一个不变阈值 （3）在子图象中随机选取一点，计算它与模板中相应点的绝对误差值， 将每一随机点对的误差累加起来，若累加到第r次时误差超过设定阈值，则停止累加，记下此时的累加次数r （4） 对于整 幅图像计算误差e，可得到一个由r值构成的曲面，曲面最大值处对应的位置即为模板最佳匹配位置。这是因为该点需要多次累加误差才能超过阈值，因此相对于其它点，它最有可能是匹配位置。 推导 假设：S(x,y)是MxN的搜索图，T(x,y)是mxn的模板图，是搜索图中的一个子图（左上角起始位置为(i,j)）。则存在 1≤i⩽m−M−1,1≤j≥n−N−11\\leq i\\leqslant m-M-1,1\\leq j\\geq n-N-1 1≤i⩽m−M−1,1≤j≥n−N−1 SSDA算法描述如下： ①定义绝对误差： ε(i,j,s,t)=∣Si,j(s,t)−Si,j‾−T(s,t)+T‾∣\\varepsilon ( i,j,s,t )=\\mid S_{i,j}(s,t)-\\overline{S_{i,j}}-T(s,t)+\\overline{T}| ε(i,j,s,t)=∣Si,j​(s,t)−Si,j​​−T(s,t)+T∣ 其中，带有上划线的分别表示子图、模板的均值： Si,j‾=E(Si,j)=1M∗N∑s=1M∑t=1NSi,j(s,t)\\overline{S_{i,j}}=E(S_{i,j})=\\frac{1}{M*N}\\sum_{s=1}^{M}\\sum_{t=1}^{N}S_{i,j}(s,t) Si,j​​=E(Si,j​)=M∗N1​s=1∑M​t=1∑N​Si,j​(s,t) T‾=E(T)=1M∗N∑s=1M∑t=1NT(s,t)\\overline{T}=E(T)=\\frac{1}{M*N}\\sum_{s=1}^{M}\\sum_{t=1}^{N}T(s,t) T=E(T)=M∗N1​s=1∑M​t=1∑N​T(s,t) 实际上，绝对误差就是子图与模板图各自去掉其均值后，对应位置之差的绝对值。 ②设定阈值Th； ③在模板图中随机选取不重复的像素点，计算与当前子图的绝对误差，将误差累加，当误差累加值超过了Th时，记下累加次数H，所有子图的累加次数H用一个表R(i,j)来表示。SSDA检测定义为： R(i,j)=H∣min1⩽H⩽M∗N[∑h−1Hε(i,j,s,t)⩾Th]R(i,j)={H|\\underset{1\\leqslant H\\leqslant M*N}{min}\\left [\\sum_{h-1}^{H}\\varepsilon ( i,j,s,t )\\geqslant Th \\right ]} R(i,j)=H∣1⩽H⩽M∗Nmin​[h−1∑H​ε(i,j,s,t)⩾Th] 下图给出了A、B、C三点的误差累计增长曲线，其中A、B两点偏离模板，误差增长得快；C点增长缓慢，说明很可能是匹配点（图中Tk相当于上述的Th，即阈值；I(i,j)相当于上述R(i,j)，即累加次数）。 ④在计算过程中，随机点的累加误差和超过了阈值（记录累加次数H）后，则放弃当前子图转而对下一个子图进行计算。遍历完所有子图后，选取最大R值所对应的(i,j)子图作为匹配图像【若R存在多个最大值（一般不存在），则取累加误差最小的作为匹配图像】。 由于随机点累加值超过阈值Th后便结束当前子图的计算，所以不需要计算子图所有像素，大大提高了算法速度；为进一步提高速度，可以先进行粗配准，即：隔行、隔离的选取子图，用上述算法进行粗糙的定位，然后再对定位到的子图，用同样的方法求其8个邻域子图的最大R值作为最终配准图像。这样可以有效的减少子图个数，减少计算量，提高计算速度。 ","link":"https://kily-007.github.io/post/序贯相似性检测算法-SSDA/"},{"title":"kafka自定义序列化类","content":"kafka版本 zookeeper使用版本：zookeeper-3.4.14.tar.gz kafka使用版本：kafka_2.12-2.0.0.tgz pom.xml依赖： &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.12&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt; kafka原生序列化 kafka在发送和接受消息的时候，都是以byte[]字节型数组发送或者接受的。但是我们平常使用的时候，不但可以使用byte[]，还可以使用int、short、long、float、double、String等数据类型，这是因为在我们使用这些数据类型的时候，kafka根据我们指定的序列化和反序列化方式转成byte[]类型之后再进行发送或者接受的。 通常我们在使用kakfa发送或者接受消息的时候都需要指定消息的key和value序列化方式 ,例如： config.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); config.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); kafka序列化方式 序列化方式 对应java数据类型 说明 org.apache.kafka.common.serialization.ByteArraySerializer byte[] 原生类型 org.apache.kafka.common.serialization.ByteBufferSerializer ByteBuffer 关于ByteBuffer org.apache.kafka.common.serialization.IntegerSerializer Interger org.apache.kafka.common.serialization.ShortSerializer Short org.apache.kafka.common.serialization.LongSerializer Long org.apache.kafka.common.serialization.DoubleSerializer Double org.apache.kafka.common.serialization.StringSerializer String kafka反序列化方式 反序列化方式 对应java数据类型 说明 org.apache.kafka.common.serialization.ByteArrayDeserializer byte[] 原生类型 org.apache.kafka.common.serialization.ByteBufferDeserializer ByteBuffer 关于ByteBuffer org.apache.kafka.common.serialization.IntegerDeserializer Interger org.apache.kafka.common.serialization.ShortDeserializer Short org.apache.kafka.common.serialization.LongDeserializer Long org.apache.kafka.common.serialization.DoubleDeserializer Double org.apache.kafka.common.serialization.StringDeserializer String kafka原生序列化实现 以String为例子，我们分析kafka如何实现序列化与反序列化 String序列化 查看 org.apache.kafka.common.serialization.StringSerializer 类 /* * Licensed to the Apache Software Foundation (ASF) under one or more * contributor license agreements. See the NOTICE file distributed with * this work for additional information regarding copyright ownership. * The ASF licenses this file to You under the Apache License, Version 2.0 * (the &quot;License&quot;); you may not use this file except in compliance with * the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ package org.apache.kafka.common.serialization; import org.apache.kafka.common.errors.SerializationException; import java.io.UnsupportedEncodingException; import java.util.Map; /** * String encoding defaults to UTF8 and can be customized by setting the property key.serializer.encoding, * value.serializer.encoding or serializer.encoding. The first two take precedence over the last. */ public class StringSerializer implements Serializer&lt;String&gt; { private String encoding = &quot;UTF8&quot;; @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) { String propertyName = isKey ? &quot;key.serializer.encoding&quot; : &quot;value.serializer.encoding&quot;; Object encodingValue = configs.get(propertyName); if (encodingValue == null) encodingValue = configs.get(&quot;serializer.encoding&quot;); if (encodingValue instanceof String) encoding = (String) encodingValue; } @Override public byte[] serialize(String topic, String data) { try { if (data == null) return null; else return data.getBytes(encoding); } catch (UnsupportedEncodingException e) { throw new SerializationException(&quot;Error when serializing string to byte[] due to unsupported encoding &quot; + encoding); } } } String的序列化类是继承了Serializer接口，指定泛型，然后实现的Serializer接口的configure()、serialize()方法。代码重点的实现是在serialize()，可以看出这个方法将我们传入的String类型的数据，简单的通过data.getBytes()方法进行了序列化。 string反序列化 查看 org.apache.kafka.common.serialization.StringDeserializer类 /* * Licensed to the Apache Software Foundation (ASF) under one or more * contributor license agreements. See the NOTICE file distributed with * this work for additional information regarding copyright ownership. * The ASF licenses this file to You under the Apache License, Version 2.0 * (the &quot;License&quot;); you may not use this file except in compliance with * the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ package org.apache.kafka.common.serialization; import org.apache.kafka.common.errors.SerializationException; import java.io.UnsupportedEncodingException; import java.util.Map; /** * String encoding defaults to UTF8 and can be customized by setting the property key.deserializer.encoding, * value.deserializer.encoding or deserializer.encoding. The first two take precedence over the last. */ public class StringDeserializer implements Deserializer&lt;String&gt; { private String encoding = &quot;UTF8&quot;; @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) { String propertyName = isKey ? &quot;key.deserializer.encoding&quot; : &quot;value.deserializer.encoding&quot;; Object encodingValue = configs.get(propertyName); if (encodingValue == null) encodingValue = configs.get(&quot;deserializer.encoding&quot;); if (encodingValue instanceof String) encoding = (String) encodingValue; } @Override public String deserialize(String topic, byte[] data) { try { if (data == null) return null; else return new String(data, encoding); } catch (UnsupportedEncodingException e) { throw new SerializationException(&quot;Error when deserializing byte[] to string due to unsupported encoding &quot; + encoding); } } } String的反序列化类是继承了Deserializer接口，指定泛型，然后实现的Deserializer接口的configure()、deserialize()方法。代码重点的实现是在deserialize()，可以看出这个方法将我们传入的byte[]类型的数据，简单的通过return new String(data, encoding)方法进行了反序列化得到了String类型的数据。 float序列化 我们再看float类型数据的序列化类型，查看org.apache.kafka.common.serialization.FloatSerializer类 /* * Licensed to the Apache Software Foundation (ASF) under one or more * contributor license agreements. See the NOTICE file distributed with * this work for additional information regarding copyright ownership. * The ASF licenses this file to You under the Apache License, Version 2.0 * (the &quot;License&quot;); you may not use this file except in compliance with * the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ package org.apache.kafka.common.serialization; public class FloatSerializer implements Serializer&lt;Float&gt; { @Override public byte[] serialize(final String topic, final Float data) { if (data == null) return null; long bits = Float.floatToRawIntBits(data); return new byte[] { (byte) (bits &gt;&gt;&gt; 24), (byte) (bits &gt;&gt;&gt; 16), (byte) (bits &gt;&gt;&gt; 8), (byte) bits }; } } 可以看到，FloatSerializer类继承了Serializer接口，指定泛型，然后实现的Serializer接口的serialize()方法，即直接将float类型数据转为byte数组返回。 float反序列化 查看org.apache.kafka.common.serialization.FloatSerializer类 /* * Licensed to the Apache Software Foundation (ASF) under one or more * contributor license agreements. See the NOTICE file distributed with * this work for additional information regarding copyright ownership. * The ASF licenses this file to You under the Apache License, Version 2.0 * (the &quot;License&quot;); you may not use this file except in compliance with * the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ package org.apache.kafka.common.serialization; import org.apache.kafka.common.errors.SerializationException; public class FloatDeserializer implements Deserializer&lt;Float&gt; { @Override public Float deserialize(final String topic, final byte[] data) { if (data == null) return null; if (data.length != 4) { throw new SerializationException(&quot;Size of data received by Deserializer is not 4&quot;); } int value = 0; for (byte b : data) { value &lt;&lt;= 8; value |= b &amp; 0xFF; } return Float.intBitsToFloat(value); } } 可以看到，FloatSerializer类继承了Serializer接口，指定泛型，然后实现的Serializer接口的serialize()方法，即直接将byte数组转回为float类型返回。 自此我们可以确定，kafka中的序列化方式就是将数据转为byte[]数组方式进行传递，在消费者端通过反序列化类解析byte[]数组方式来解析数据，下面我们来自己定义序列化类。 kakfa自定义序列化类 基于fastjson的序列化方式 配置pom.xml中引入依赖： &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.37&lt;/version&gt; &lt;/dependency&gt; student对象实体 /** * * Title: Student.java * Description: TODO * * @author MrChen * @date 2019年12月5日 上午11:22:38 */ public class Student { int id; String name; int age; boolean sex; float[] sorce; public Student(int id, String name, int age, boolean sex, float[] sorce) { super(); this.id = id; this.name = name; this.age = age; this.sex = sex; this.sorce = sorce; } public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } public boolean isSex() { return sex; } public void setSex(boolean sex) { this.sex = sex; } public float[] getSorce() { return sorce; } public void setSorce(float[] sorce) { this.sorce = sorce; } } 序列化实现StudentSerializer /** * * Title: StudentSerializer.java * Description: student序列化类 * * @author MrChen * @date 2019年12月4日 下午5:13:41 */ public class StudentSerializer implements Serializer&lt;Student&gt;{ @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) { // TODO Auto-generated method stub } @Override public byte[] serialize(String topic, Student data) { // TODO Auto-generated method stub return JSON.toJSONBytes(data); } @Override public void close() { // TODO Auto-generated method stub } } 反序列化实现StudentDeserializer /** * * Title: StudentDeserializer.java * Description: student反序列化类 * * @author MrChen * @date 2019年12月4日 下午5:13:11 */ public class StudentDeserializer implements Deserializer&lt;Student&gt;{ @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) { // TODO Auto-generated method stub } @Override public DemodulatorParameters deserialize(String topic, byte[] data) { // TODO Auto-generated method stub return JSON.parseObject(data, Student.class); } @Override public void close() { // TODO Auto-generated method stub } } 基于protostuff的序列化方式 工具类ProtostuffUtil import com.dyuproject.protostuff.LinkedBuffer; import com.dyuproject.protostuff.ProtostuffIOUtil; import com.dyuproject.protostuff.Schema; import com.dyuproject.protostuff.runtime.RuntimeSchema; import java.util.Map; import java.util.concurrent.ConcurrentHashMap; /** * Created by shirukai on 2018/8/14 * protostuff 序列化/反序列化工具类 */ public class ProtostuffUtil { private static Map&lt;Class&lt;?&gt;, Schema&lt;?&gt;&gt; cachedSchema = new ConcurrentHashMap&lt;&gt;(); /** * 序列化 * * @param message 序列化数据 * @param tClass .class * @param &lt;T&gt; 类型 * @return byte[] */ public static &lt;T&gt; byte[] serializer(T message, Class&lt;T&gt; tClass) { Schema&lt;T&gt; schema = getSchema(tClass); return ProtostuffIOUtil.toByteArray(message, schema, LinkedBuffer.allocate(LinkedBuffer.DEFAULT_BUFFER_SIZE)); } /** * 反序列化 * * @param bytes bytes * @param tClass .class * @param &lt;T&gt; 类型 * @return T */ public static &lt;T&gt; T deserializer(byte[] bytes, Class&lt;T&gt; tClass) { Schema&lt;T&gt; schema = getSchema(tClass); T message = schema.newMessage(); ProtostuffIOUtil.mergeFrom(bytes, message, schema); return message; } private static &lt;T&gt; Schema&lt;T&gt; getSchema(Class&lt;T&gt; tClass) { Schema&lt;T&gt; schema = (Schema&lt;T&gt;) cachedSchema.get(tClass); if (schema == null) { schema = RuntimeSchema.createFrom(tClass); cachedSchema.put(tClass, schema); } return schema; } } 序列化实现StudentProtostuffSerializer import com.springboot.demo.kafka.entity.Person; import com.springboot.demo.utils.ProtostuffUtil; import org.apache.kafka.common.serialization.Serializer; import java.util.Map; /** * Created by shirukai on 2018/8/25 */ public class StudentProtostuffSerializer implements Serializer&lt;Student&gt; { @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) { } @Override public byte[] serialize(String topic, Student data) { return ProtostuffUtil.serializer(data, Person.class); } @Override public void close() { } } 反序列化实现StudentProtostuffDeserializer import com.springboot.demo.kafka.entity.Person; import com.springboot.demo.utils.ProtostuffUtil; import org.apache.kafka.common.serialization.Deserializer; import java.util.Map; /** * Created by shirukai on 2018/8/25 */ public class StudentProtostuffDeserializer implements Deserializer&lt;Student&gt; { @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) { } @Override public Person deserialize(String topic, byte[] data) { return ProtostuffUtil.deserializer(data, Student.class); } @Override public void close() { } } 总结 根据自己的需求对kafka消息进行自定义序列化，将数据信息转为byte[]数组的形式，在反序列化端将byte[]反向转为数据信息。 ","link":"https://kily-007.github.io/post/kafka自定义序列化类/"},{"title":"springboot数据库访问","content":"使用jpa(JpaRepository)方式连接访问数据库 使用jpa(JpaRepository)方式连接访问数据库 配置文件 在application.yml中配置连接mysql数据库 datasource: url: jdbc:mysql://localhost:3306/springbootdb?serverTimezone=CTT username: root password: root driver-class-name: com.mysql.cj.jdbc.Driver 在pom.xml中添加jpa配置 &lt;!-- Spring Data JPA 依赖 :: 数据持久层框架 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; 数据库访问类 package edu.whut.demo.mapper; import edu.whut.demo.bean.User; import org.springframework.data.domain.Page; import org.springframework.data.domain.Pageable; import org.springframework.data.jpa.repository.JpaRepository; import java.util.List; //继承JpaRepository public interface UserMapper extends JpaRepository&lt;User,String&gt; { //不需要写sql语句，方法名根据规范撰写，jps根据方法名解析出SQL语句 //根据用户名查找用户 User findUserByUsername(String username); //分页查询 Page&lt;User&gt; findAll(Pageable pageable); //查找所有用户 List&lt;User&gt; findAll(); } ","link":"https://kily-007.github.io/post/springboot数据库访问/"},{"title":"springboot之JpaRepository查询列名错误","content":"记录一次报错。 java.sql.SQLSyntaxErrorException: Unknown column 'subwaycoun0_.pass_num1' in 'field list' at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[mysql-connector-java-8.0.18.jar:8.0.18] at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97) ~[mysql-connector-java-8.0.18.jar:8.0.18] 记录一次报错。 在数据库查询中，对实体类使用JpaRepository，如用户表 -- 用户表 CREATE TABLE IF NOT EXISTS USER (id INT UNSIGNED AUTO_INCREMENT, userName VARCHAR(20) NOT NULL, passWord VARCHAR(32) NOT NULL, sex VARCHAR(4) NOT NULL, phone VARCHAR(32), email VARCHAR(32), employer VARCHAR(32), remarks VARCHAR(1000), PRIMARY KEY (id) )ENGINE=INNODB DEFAULT CHARSET=utf8; //user类 public class User { @Id @Column private String id; @Column private String userName; @Column private String passWord; @Column private String sex; @Column private String phone; @Column private String email; @Column private String employer; @Column private String remarks; 。。。。 } //数据库交互类 import com.example.springboot.bean.User; import org.springframework.data.domain.Page; import org.springframework.data.domain.Pageable; import org.springframework.data.jpa.repository.JpaRepository; import java.util.List; public interface UserMapper extends JpaRepository&lt;User,String&gt; { User findUserByUsername(String username); Page&lt;User&gt; findAll(Pageable pageable); List&lt;User&gt; findAll(); } 此时在调用查找时会报错如下： java.sql.SQLSyntaxErrorException: Unknown column 'subwaycoun0_.pass_num1' in 'field list' at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120) ~[mysql-connector-java-8.0.18.jar:8.0.18] at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97) ~[mysql-connector-java-8.0.18.jar:8.0.18] 原因：主要就是数据库和实体类名称不匹配，造成不匹配的原因为数据库字段时驼峰命名的userName执行的时候他会给你的查询字段名称改为user_Name 解决办法1：将数据库中的驼峰命名法改为下划线，即:userName -&gt; user_name ","link":"https://kily-007.github.io/post/springboot之JpaRepository查询列名错误/"},{"title":"前端框架layui","content":" 第一次发现它是同学推荐给我的，发现真的非常方便，layui网址：https://www.layui.com/，下面简单介绍和记录一下。 准备工作 将layui下载下来，其中包含css/font/images/lay和layui.js四个文件夹和一个js文件。 参考官方文档和示例，找你需要的模块。 下面记录一下几个用过的模块。 layui后台布局 后台布局的效果如图所示，直接点击获取布局代码幅值过去改下路径就能用。 layui数据表格 记录一个bug 数据表对应的js代码为： layui.use('table', function(){ var table = layui.table; table.render({ elem: '#test' ,url:'/demo/table/user/' ,cellMinWidth: 80 //全局定义常规单元格的最小宽度，layui 2.2.1 新增 ,cols: [[ {field:'id', width:80, title: 'ID', sort: true} ,{field:'username', width:80, title: '用户名'} ,{field:'sex', width:80, title: '性别', sort: true} ,{field:'city', width:80, title: '城市'} ,{field:'sign', title: '签名', width: '30%', minWidth: 100} //minWidth：局部定义当前单元格的最小宽度，layui 2.2.1 新增 ,{field:'experience', title: '积分', sort: true} ,{field:'score', title: '评分', sort: true} ,{field:'classify', title: '职业'} ,{field:'wealth', width:137, title: '财富', sort: true} ]] ,page: true ,response: { statusCode: 200 //重新规定成功的状态码为 200，table 组件默认为 0 } ,parseData: function(res){ //将原始数据解析成 table 组件所规定的数据 return { &quot;code&quot;: res.status, //解析接口状态 &quot;msg&quot;: res.message, //解析提示文本 &quot;count&quot;: res.total, //解析数据长度 &quot;data&quot;: res.data.content //解析数据列表 }; } }); }); 在controller中返回数据结构类为： package com.example.springboot.utils; public class LayuiTableResultUtil&lt;T&gt; { private T data; private String msg; private int code; private Long count; public LayuiTableResultUtil(int code,String msg, Long count, T data) { this.msg = msg; this.data = data; this.code = code; this.count = count; } @Override public String toString() { return &quot;LayuiTableResult [msg=&quot; + msg + &quot;, data=&quot; + data + &quot;, code=&quot; + code + &quot;, count=&quot; + count + &quot;]&quot;; } public String getMsg() { return msg; } public void setMsg(String msg) { this.msg = msg; } public Long getCount() { return count; } public void setCount(Long count) { this.count = count; } public int getCode() { return code; } public void setCode(int code) { this.code = code; } public T getData() { return data; } public void setData(T data) { this.data = data; } } 项目启动，返回页面上显示json字符串，无法解析数据；调了三天也没解决，然后换Thymeleaf显示了。 { &quot;code&quot;:0, &quot;msg&quot;:&quot;&quot;, &quot;count&quot;:100, &quot;data&quot;:{&quot;content&quot;:[ {&quot;id&quot;: 1, &quot;username&quot;: &quot;user1&quot;, &quot;sex&quot;: &quot;女&quot;, &quot;city&quot;: &quot;城市-0&quot;, &quot;sign&quot;: &quot;签名-0&quot;, ...} {&quot;id&quot;: 1, &quot;username&quot;: &quot;user1&quot;, &quot;sex&quot;: &quot;女&quot;, &quot;city&quot;: &quot;城市-0&quot;, &quot;sign&quot;: &quot;签名-0&quot;, ...} {&quot;id&quot;: 1, &quot;username&quot;: &quot;user1&quot;, &quot;sex&quot;: &quot;女&quot;, &quot;city&quot;: &quot;城市-0&quot;, &quot;sign&quot;: &quot;签名-0&quot;, ...} ... ]} } ","link":"https://kily-007.github.io/post/前端框架layui/"},{"title":"springboot","content":"springboot项目结构： controller:控制器层，导入service层，调用service方法；通过接受前端传过来的参数进行业务操作，再返回一个指定的路径或数据表 bean/entity/model:实体类，映射数据库中的字段 mapper/dao:放的是对数据库的操作，如CRUD service:业务层，存放业务逻辑处理；不直接对数据库进行操作，有接口和接口实现类；提供controller层调用的方法 controller:控制器层，导入service层，调用service方法；通过接受前端传过来的参数进行业务操作，再返回一个指定的路径或数据表 bean/entity/model:实体类，映射数据库中的字段 mapper/dao:放的是对数据库的操作，如CRUD service:业务层，存放业务逻辑处理；不直接对数据库进行操作，有接口和接口实现类；提供controller层调用的方法 springboot模板类型 Spring Boot提供了默认配置的模板引擎主要有以下几种： Thymeleaf（官方推荐） FreeMarker Velocity Groovy Mustache JSP 模板Thymeleaf 到pom.xml文件中添加依赖 &lt;!-- 模板引擎 Thymeleaf 依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; 配置application.yml文件 spring: thymeleaf: prefix: classpath: /templates/ check-template-location: true cache: false suffix: .html encoding: UTF-8 mode: HTML5 servlet: content-type: text/html 传参 @Controller @RequestMapping(&quot;/thymeleaf&quot;) public class ThymeleafController { @RequestMapping(&quot;&quot;) public ModelAndView index(){ List&lt;User&gt; userList = new ArrayList&lt;User&gt;(); User user1 = new User(&quot;solidwang&quot;, &quot;solidwang@126.com&quot;); User user2 = new User(&quot;jobs&quot;, &quot;jobs@me.com&quot;); userList.add(user1); userList.add(user2); ModelAndView modelAndView = new ModelAndView(&quot;/index&quot;); modelAndView.addObject(&quot;userList&quot;, userList); return modelAndView; } } &lt;!DOCTYPE html&gt; &lt;html xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt; &lt;head&gt; &lt;title&gt;learn Resources&lt;/title&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt; &lt;h1&gt;Thymeleaf测试&lt;/h1&gt; &lt;table border=&quot;1&quot; cellspacing=&quot;1&quot; cellpadding=&quot;0&quot;&gt; &lt;tr&gt; &lt;td&gt;姓名&lt;/td&gt; &lt;td&gt;passport&lt;/td&gt; &lt;/tr&gt; &lt;tr th:each=&quot;user : ${userList}&quot;&gt; &lt;td th:text=&quot;${user.username}&quot;&gt;solidwang&lt;/td&gt; &lt;td th:text=&quot;${user.passport}&quot;&gt;solidwang@me.com&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; ","link":"https://kily-007.github.io/post/springboot/"},{"title":"windows远程连接桌面linux","content":"准备工作 1）centos桌面已安装 2）windows系统 centos安装配置xrdp 安装epel库 检测是否已安装epel库 rpm -qa |grep epel 安装epel库 yum install epel-release 在安装过程中会出现选择，直接选择Y即可. 安装xrdp yum install xrdp 安装vnc 因为Xrdp最终会自动启用VNC，所以必须安装tigervnc-server，否则xrdp无法使用。安装vnc： yum install tigervnc-server 为root用户设置VNC密码 vncpasswd root 修改xrdp最大连接数 （使用默认值，不修改也是可以的） vim /etc/xrdp/xrdp.ini （默认是32） 关闭防火墙 systemctl stop firewalld.service 启动xrdp systemctl start xrdp 设置开机启动 systemctl enable xrdp 远程测试连接 在windows机上开打运行，输入mstsc，输入远程linux的ip ","link":"https://kily-007.github.io/post/windows远程连接桌面linux/"},{"title":"笔记-MySql主从复制原理","content":"MySql主从复制的作用 读写分离 备份 高可用 架构扩展 MySql主从形式 一主一从 一主多从，提高系统读性能。 多主一从，可以将多个数据库备份到一台性能较好的额服务器上。 双主复制，互做主从复制，一方变更，会通过复制应用到另一方数据库中。 级联复制，3-5个从节点连接主节点，其它节点作为二级或者三级节点与从节点相连。 MySql主从复制原理 MySql 主从复制涉及三个线程，一个运行在主节点(log dump thread)，两个(I/O thread,SQL thread)运行在从节点上。 log dump线程：当从节点连接主节点时，主节点创建log dump线程对bin-log加锁，读取内容发送到从节点。 I/O线程：当从节点上执行“start skave”命令后，从节点会创建一个I/O线程用来连接主节点，请求主库中更新的bin-log。I/O线程接收到主节点binlog dump进程发来的更新之后，保存在本地relay-log中。 **SQL线程：**SQL线程负责读取relay-log中的内容，解析成具体的操作并执行，最终保证主从数据的一致性。 MySQL 主从复制默认是异步的模式。MySQL增删改操作会全部记录在binary log中，当slave节点连接master时，会主动从master处获取最新的bin log文件。并把bin log中的sql relay。 ","link":"https://kily-007.github.io/post/笔记-MySql主从复制原理/"},{"title":"笔记-java创建线程的几种方式","content":"Java使用Thread类代表线程，所有的线程对象都必须是Thread类或其子类的实例。Java可以用四种方式来创建线程，如下： 1）继承Thread类创建线程； 2）实现Runnable接口创建线程； 3）实现Callable接口通过FutureTask包装器来创建Thread线程； 4）线程池 继承Thread类 通过继承Thread类来创建并启动多线程的一般步骤如下： 1】定义Thread类的子类，并重写该类的run()方法，该方法的方法体就是线程需要完成的任务，run()方法也称为线程执行体； 2】创建Thread子类的实例，也就是创建了线程对象； 3】启动线程，即调用线程的start()方法。 public class MyThread extends Thread{ public void run() { for (int i = 0; i &lt; 10; i++) { System.out.println(Thread.currentThread()+&quot;:&quot;+i); } } public static void main(String[] args) { MyThread thread1=new MyThread(); MyThread thread2=new MyThread(); MyThread thread3=new MyThread(); thread1.start(); thread2.start(); thread3.start(); } } 实现Runnable接口 通过实现Runnable接口创建并启动线程一般步骤如下： 1】定义Runnable接口的实现类，一样要重写run()方法，这个run()方法和Thread中的run()方法一样是线程的执行体； 2】创建Runnable实现类的实例，并用这个实例作为Thread的target来创建Thread对象，这个Thread对象才是真正的线程对象； 3】第三步依然是通过调用线程对象的start()方法来启动线程。 public class MyThread implements Runnable{ public void run() { for (int i = 0; i &lt; 10; i++) { System.out.println(Thread.currentThread().getName()+&quot;:&quot;+i); } } public static void main(String[] args) { MyThread myThread1=new MyThread(); MyThread myThread2=new MyThread(); MyThread myThread3=new MyThread(); Thread thread1=new Thread(myThread1); Thread thread2=new Thread(myThread2); Thread thread3=new Thread(myThread3); thread1.start(); thread2.start(); thread3.start(); } } 通过Callble和future Task 和Runnable接口不一样，Callable接口提供了一个call() 方法作为线程执行体，call() 方法比run() 方法功能要强大： call() 方法可以有返回值； call() 方法可以声明抛出异常。 Java5提供了Future接口来代表Callable接口里call() 方法的返回值，并且为Future接口提供了一个实现类FutureTask，这个实现类既实现了Future接口，还实现了Runnable接口，因此可以作为Thread类的target。在Future接口里定义了几个公共方法来控制它关联的Callable任务。 boolean cancel(boolean mayInterruptIfRunning)：视图取消该Future里面关联的Callable任务； get()：返回Callable里call() 方法的返回值，调用这个方法会导致程序阻塞，必须等到子线程结束后才会得到返回值； get(long timeout,TimeUnit unit)：返回Callable里call() 方法的返回值，最多阻塞timeout时间，经过指定时间没有返回抛出TimeoutException； boolean isDone()：若Callable任务完成，返回True； boolean isCancelled()：如果在Callable任务正常完成前被取消，返回True。 介绍了相关的概念之后，创建并启动有返回值的线程的步骤如下： 1】创建Callable接口的实现类，并实现call() 方法，然后创建该实现类的实例（从Java8开始可以直接使用Lambda表达式创建Callable对象）； 2】使用FutureTask类来包装Callable对象，该FutureTask对象封装了Callable对象的call() 方法的返回值； 3】使用FutureTask对象作为Thread对象的target创建并启动线程（因为FutureTask实现了Runnable接口）； 4】调用FutureTask对象的get() 方法来获得子线程执行结束后的返回值。 import java.util.concurrent.Callable; import java.util.concurrent.ExecutionException; import java.util.concurrent.FutureTask; public class MyThread implements Callable&lt;String&gt;{ private int count=20; @Override public String call() throws Exception { for (int i = 20; i &gt;0; i--) { System.out.println(Thread.currentThread().getName()+&quot;当前票数&quot;+i); } return &quot;sale out&quot;; } public static void main(String[] args) throws InterruptedException, ExecutionException { Callable&lt;String&gt; callable =new MyThread(); FutureTask &lt;String&gt;futureTask=new FutureTask&lt;&gt;(callable); Thread thread1=new Thread(futureTask); Thread thread2=new Thread(futureTask); Thread thread3=new Thread(futureTask); thread1.start(); thread2.start(); thread3.start(); System.out.println(futureTask.get()); } } 使用线程池 Executor Java通过Executors提供四种线程池，分别为： FixedThreadPool：返回固定线程数量的线程池。 SingleThreadExecutor：返回一个只有一个线程的线程池。 CachedThreadPool：该方法返回一个可根据实际情况调整线程数量的线程池。线程池的线程数量不确定，但 若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新 的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。 ScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。 ThreadPoolExecutor **Executors是JAVA并发包中提供的，用来快速创建不同类型的线程池。***但在一些大型项目中，这种做法一般是禁止的。因为用Executors创建的线程池存在性能隐患，我们看一下源码就知道，用Executors创建线程池时，使用的队列是new LinkedBlockingQueue&lt;Runnable&gt;()，这是一个无边界队列，如果不断的往里加任务时，最终会导致内存问题，也就是说在项目中由于使用了无边界队列，导致的内存占用的不可控性。故一般采用自己实现ThreadPoolExecutor的方式来自定义线程池。 ThreadPoolExecutor( int corePoolSize,//指定线程池中线程的数量 int maximumPoolSize,//最大线程数量 long keepAliveTime,//超过核心线程数的线程超过corePoolSize时，多余线程多久被销毁 TimeUnit unit,//KeepAliveTime的单位 BlockingQueue&lt;Runnable&gt; workQueue,//任务队列 ThreadFactory threadFactory,//线程工厂 RejectedExecutionHandler handler//拒绝策略 ) 设线程池中的线程数量为n，，当一个任务通过execute方法添加到线程池时： 若n&lt;corePoolSize，创建新线程来处理； 若n=corePoolSize，但缓冲队列workQueue未满，那么任务放进队列； 若n&gt;corePoolSize，缓冲队列workQueue满，且n&lt;maximumPoolSize，新建线程处理任务； 若n&gt;corePoolSize，缓冲队列workQueue满，且n=maximumPoolSize，通过handle指定的拒绝策略来处理任务。 任务处理优先级为：核心线程corePoolSize、任务队列workQueue、最大线程maximumPoolSize，如果三者都满了，使用handler处理被拒绝的任务。 拒绝策略 在分析ThreadPoolExecutor的构造参数时，有一个RejectedExecutionHandler参数。RejectedExecutionHandler是一个接口： public interface RejectedExecutionHandler { void rejectedExecution(Runnable r, ThreadPoolExecutor executor); } 里面只有一个方法。当要创建的线程数量大于线程池的最大线程数的时候，新的任务就会被拒绝，就会调用这个接口里的这个方法。可以自己实现这个接口，实现对这些超出数量的任务的处理。 ThreadPoolExecutor自己已经提供了四个拒绝策略，分别是AbortPolicy,CallerRunsPolicy,DiscardPolicy,DiscardOldestPolicy AbortPolicy：ThreadPoolExecutor中默认的拒绝策略就是AbortPolicy。直接抛出异常。 CallerRunsPolicy:在任务被拒绝添加后，会调用当前线程池的所在的线程去执行被拒绝的任务。 DiscardPolicy:采用这个拒绝策略，会让被线程池拒绝的任务直接抛弃，不会抛异常也不会执行。 DiscardOldestPolicy：当任务被拒绝添加时，会抛弃任务队列中最旧的任务也就是最先加入队列的，再把这个新任务添加进去。 ","link":"https://kily-007.github.io/post/笔记-java创建线程的几种方式/"},{"title":"算法-求全排列","content":"题目：数字1，2，3，4全排列。 全排列 思路：使用递归策略将待排数列逐一缩小。例如1,2,3,4。先把1固定，递归求2,3,4的全排列；又把2固定，递归求3,4全排列......直到只剩一个数，输出这个排列。图解如下： private static void permutation(int[] array, int index) { if (index == array.length - 1) System.out.println(Arrays.toString(array)); //从固定的数，与后面每一位交换 for (int i = index; i &lt; array.length; i++) { swap(array, i, index); permutation(array, index + 1);//确定第index位的数值 swap(array, i, index); } } private static void swap(int[] array, int i, int index) { int temp=array[i]; array[i]=array[index]; array[index]=temp; } 带重复值的全排列 例如：1,2,3，2 当进行到index=0时，固定{1}+P(2,3,2)，接下来求子串(2,3,2)的全排列，但其中有两个2，无论哪个与此时的1交换，后面的子串都相同，会重复。故需要跳过保留其中一种就可以了，查询是否重复的区间为当前进行到的位置index和准备交换的位置i，在i之前若有重复的，则说明index位置元素不需要再与i位置交换计算了。代码如下： private static void permutation(int[] array, int index) { if (index == array.length - 1) System.out.println(Arrays.toString(array)); // 用当前位值，与后面每一位交换 for (int i = index; i &lt; array.length; i++) { if(check(array,index,i)){ swap(array, i, index); permutation(array, index + 1);//确定第index位的数值 swap(array, i, index); } } } private static boolean check(int[] array, int index, int i) { if (i &gt; index) {//兼容index=i的情况 for (int j = index; j &lt; i; j++) { if (array[j] == array[i]) return false; } } return true; } private static void swap(int[] array, int i, int index) { int temp = array[i]; array[i] = array[index]; array[index] = temp; } ","link":"https://kily-007.github.io/post/算法-求全排列/"},{"title":"算法-KMP","content":"KMP算法 暴力匹配 问题：有一个文本串S，和一个模式串P，现在要查找P在S中的位置，直接思路：暴力查找。 public int strMatch(String S,String P){ int i=0; int j=0; while(i&lt;S.length()&amp;&amp;j&lt;P.length()){ if(S[i]==P[j]){ i++; j++; }else{ i=i-j+1; j=0; } } if(j==P.length()){ return i-j; }else{ return -1; } } KMP算法 假设现在文本串S匹配到 i 位置，模式串P匹配到 j 位置 如果j = -1，或者当前字符匹配成功（即S[i] == P[j]），都令i++，j++，继续匹配下一个字符； 如果j != -1，且当前字符匹配失败（即S[i] != P[j]），则令 i 不变，j = next[j]。此举意味着失配时，模式串P相对于文本串S向右移动了j - next [j] 位。则代码变为： public int strMatch(String S,String P){ int i=0; int j=0; while(i&lt;S.length()&amp;&amp;j&lt;P.length()){ if(j==-1||S[i]==P[j]){ i++; j++; }else{ j=next[j]; } } if(j==P.length()){ return i-j; }else{ return -1; } } 当S[10]跟P[6]匹配失败时，KMP不是跟暴力匹配那样简单的把模式串右移一位，而是执行第②条指令：“如果j != -1，且当前字符匹配失败（即S[i] != P[j]），则令 i 不变，j = next[j]”，即j 从6变到2（后面我们将求得P[6]，即字符D对应的next 值为2），所以相当于模式串向右移动的位数为j - next[j]（j - next[j] = 6-2 = 4）。 求next数组 next 数组的求解：就是找最大对称长度的前缀后缀，然后整体右移一位，初值赋为-1（当然，你也可以直接计算某个字符对应的next值，就是看这个字符之前的字符串中有多大长度的相同前缀后缀）。 ​ 换言之，对于给定的模式串：ABCDABD，它的最大长度表及next 数组分别如下： 根据最大长度表求出了next 数组后，从而有失配时，模式串向右移动的位数为：失配字符所在位置 - 失配字符对应的next 值. public int[] getNextJ(int[] next){ int k=-1; int j=0; while(j&lt;next.length{ if(k==-1||p[k]==p[j]){ k++; j++; next[j]=k; }else{ k=next[k]; } } } BM算法 KMP的匹配是从模式串的开头开始匹配的，而1977年，德克萨斯大学的Robert S. Boyer教授和J Strother Moore教授发明了一种新的字符串匹配算法：Boyer-Moore算法，简称BM算法。该算法从模式串的尾部开始匹配，且拥有在最坏情况下O(N)的时间复杂度。在实践中，比KMP算法的实际效能高。 ​ BM算法定义了两个规则： 坏字符规则：当文本串中的某个字符跟模式串的某个字符不匹配时，我们称文本串中的这个失配字符为坏字符，此时模式串需要向右移动，移动的位数 = 坏字符在模式串中的位置 - 坏字符在模式串中最右出现的位置。此外，如果&quot;坏字符&quot;不包含在模式串之中，则最右出现位置为-1。 好后缀规则：当字符失配时，后移位数 = 好后缀在模式串中的位置 - 好后缀在模式串上一次出现的位置，且如果好后缀在模式串中没有再次出现，则为-1。 下面举例说明BM算法。例如，给定文本串“HERE IS A SIMPLE EXAMPLE”，和模式串“EXAMPLE”，现要查找模式串是否在文本串中，如果存在，返回模式串在文本串中的位置。 ​ 1. 首先，&quot;文本串&quot;与&quot;模式串&quot;头部对齐，从尾部开始比较。&quot;S&quot;与&quot;E&quot;不匹配。这时，&quot;S&quot;就被称为&quot;坏字符&quot;（bad character），即不匹配的字符，它对应着模式串的第6位。且&quot;S&quot;不包含在模式串&quot;EXAMPLE&quot;之中（相当于最右出现位置是-1），这意味着可以把模式串后移6-(-1)=7位，从而直接移到&quot;S&quot;的后一位。 ​ 2. 依然从尾部开始比较，发现&quot;P&quot;与&quot;E&quot;不匹配，所以&quot;P&quot;是&quot;坏字符&quot;。但是，&quot;P&quot;包含在模式串&quot;EXAMPLE&quot;之中。因为“P”这个“坏字符”对应着模式串的第6位（从0开始编号），且在模式串中的最右出现位置为4，所以，将模式串后移6-4=2位，两个&quot;P&quot;对齐。 ​ 3. 依次比较，得到 “MPLE”匹配，称为&quot;好后缀&quot;（good suffix），即所有尾部匹配的字符串。注意，&quot;MPLE&quot;、&quot;PLE&quot;、&quot;LE&quot;、&quot;E&quot;都是好后缀。 ​ 4. 发现“I”与“A”不匹配：“I”是坏字符。如果是根据坏字符规则，此时模式串应该后移2-(-1)=3位。问题是，有没有更优的移法？ ​ 5. 更优的移法是利用好后缀规则：当字符失配时，后移位数 = 好后缀在模式串中的位置 - 好后缀在模式串中上一次出现的位置，且如果好后缀在模式串中没有再次出现，则为-1。 ​ 所有的“好后缀”（MPLE、PLE、LE、E）之中，只有“E”在“EXAMPLE”的头部出现，所以后移6-0=6位。 ​ 可以看出，“坏字符规则”只能移3位，“好后缀规则”可以移6位。每次后移这两个规则之中的较大值。这两个规则的移动位数，只与模式串有关，与原文本串无关。 ​ 6. 继续从尾部开始比较，“P”与“E”不匹配，因此“P”是“坏字符”，根据“坏字符规则”，后移 6 - 4 = 2位。因为是最后一位就失配，尚未获得好后缀。 ​ 由上可知，BM算法不仅效率高，而且构思巧妙，容易理解。 Sunday算法 上文中，我们已经介绍了KMP算法和BM算法，这两个算法在最坏情况下均具有线性的查找时间。但实际上，KMP算法并不比最简单的c库函数strstr()快多少，而BM算法虽然通常比KMP算法快，但BM算法也还不是现有字符串查找算法中最快的算法，本文最后再介绍一种比BM算法更快的查找算法即Sunday算法。 ​ Sunday算法由Daniel M.Sunday在1990年提出，它的思想跟BM算法很相似： 只不过Sunday算法是从前往后匹配，在匹配失败时关注的是文本串中参加匹配的最末位字符的下一位字符。 如果该字符没有在模式串中出现则直接跳过，即移动位数 = 匹配串长度 + 1； 否则，其移动位数 = 模式串中最右端的该字符到末尾的距离+1。 ​ 下面举个例子说明下Sunday算法。假定现在要在文本串&quot;substring searching algorithm&quot;中查找模式串&quot;search&quot;。 下面举个例子说明下Sunday算法。假定现在要在文本串&quot;substring searching algorithm&quot;中查找模式串&quot;search&quot;。 ​ 1. 刚开始时，把模式串与文本串左边对齐： substring searching algorithm search 2. 结果发现在第2个字符处发现不匹配，不匹配时关注文本串中参加匹配的最末位字符的下一位字符，即标粗的字符 i，因为模式串search中并不存在i，所以模式串直接跳过一大片，向右移动位数 = 匹配串长度 + 1 = 6 + 1 = 7，从 i 之后的那个字符（即字符n）开始下一步的匹配，如下图： substring searching algorithm search ^ 3. 结果第一个字符就不匹配，再看文本串中参加匹配的最末位字符的下一位字符，是'r'，它出现在模式串中的倒数第3位，于是把模式串向右移动3位（r 到模式串末尾的距离 + 1 = 2 + 1 =3），使两个'r'对齐，如下： substring searching algorithm search 4. 匹配成功。 回顾整个过程，我们只移动了两次模式串就找到了匹配位置，缘于Sunday算法每一步的移动量都比较大，效率很高。完。 ","link":"https://kily-007.github.io/post/算法-KMP/"},{"title":"瓜子二手车面经-大数据岗","content":"瓜子二手车面经 一面： 面试官是个比较耐心的大姐姐类型，很多问题都不会也跟我说没关系，总共面了45分钟。我问了一下瓜子的开发岗都是在一起面得，内容都很相似，并没有针对特定的岗位问对应的内容，下面说下面试过程趴。 1、自我介绍和项目介绍，没撑过5分钟，让直接讲里面用到的技术栈，不要讲业务逻辑。用到的技术包括缓冲区、kafka、spark、hbase之类的。 2、java中的弱引用与强引用； 强引用是使用最普遍的引用。如果一个对象具有强引用，那垃圾回收器绝不会回收它。当内存空间不足时，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足的问题。 如果强引用对象不使用时，需要弱化从而使GC能够回收，如下： strongReference = null; 显式地设置strongReference对象为null，或让其超出对象的生命周期范围，则gc认为该对象不存在引用，这时就可以回收这个对象。具体什么时候收集这要取决于GC算法。 软引用如果一个对象只具有软引用，则内存空间充足时，垃圾回收器就不会回收它；如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。(软引用可用来实现内存敏感的高速缓存。)软引用可以和一个引用队列(ReferenceQueue)联合使用。如果软引用所引用对象被垃圾回收，JAVA虚拟机就会把这个软引用加入到与之关联的引用队列中。 当内存不足时，JVM首先将软引用中的对象引用置为null，然后通知垃圾回收器进行回收,也就是说，垃圾收集线程会在虚拟机抛出OutOfMemoryError之前回收软引用对象，而且虚拟机会尽可能优先回收长时间闲置不用的软引用对象。对那些刚构建的或刚使用过的**“较新的”软对象会被虚拟机尽可能保留**，这就是引入引用队列ReferenceQueue的原因。 应用场景： 浏览器的后退按钮。按后退时，这个后退时显示的网页内容是重新进行请求还是从缓存中取出呢？这就要看具体的实现策略了。 如果一个网页在浏览结束时就进行内容的回收，则按后退查看前面浏览过的页面时，需要重新构建； 如果将浏览过的网页存储到内存中会造成内存的大量浪费，甚至会造成内存溢出。 这时候就可以使用软引用，很好的解决了实际的问题。 弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象。 虚引用顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。虚引用主要用来跟踪对象被垃圾回收的活动。虚引用与软引用和弱引用的一个区别在于：虚引用必须和引用队列（ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 特别注意，在实际程序设计中一般很少使用弱引用与虚引用，使用软用的情况较多，这是因为软引用可以加速JVM对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生。 3、让实现LRU算法，讲讲思路； 最近最少使用算法。常用于页面置换算法，是为虚拟页式存储管理服务的。当一个数据最近一段时间没有被访问，未来被访问的概率也很小。当空间被占满后，最先淘汰最近最少使用的数据。 思路一：采用链表维持一个虚拟队列，有新元素进入时，插入到头结点；当某个结点被访问时，将其移到头结点；当超过队列容量时，删除表尾元素。 思路二：采用容器与计数的方法。元素按顺序进入容器，每个元素有一个计数器，按照进入容器的顺序计数，每被访问一次计数器加一； 4、三次握手及详细过程，什么时候调用accept()函数，有什么作用；timewaite发生在什么时候，为什么要wait？ 1.服务器调用listen进行监听。 2.客户端调用connect来发送syn报文 服务器协议栈负责三次握手的交互过程。 3.连接建立后，往listen队列中添加一个成功的连接，直到队列的最大长度。 4.服务器调用accept从listen队列中取出一条成功的tcp连接，listen队列中的连接个数就少一个 5、讲一下session和cookie的区别；在服务器上多台机器多实例需要共享session中的内容该怎么办？ session在服务端，它 的含义是指一类用来在客户端与服务器之间保持状态的解决方案。每个客户端对应一个session，每个session有一个session id， cookie在客户端，浏览器缓存访问信息、记录与资源在cookie中。 cookie的内容主要包括：名字，值，过期时间，路径和域。 由于cookie可以被人为的禁止，必须有其他机制以便在cookie被禁止时仍然能够把session id传递回服务器。经常被使用的一种技术叫做URL重写，就是把session id直接附加在URL路 径的后面，附加方式也有两种，一种是作为URL路径的附加信息，另一种是作为查询字符串附加在URL后面. 共享一般情况下，session都是存储在内存里，当服务器进程被停止或者重启的时候，内存里 的session也会被清空，如果设置了session的持久化特性，服务器就会把session保存到硬盘 上，当服务器进程重新启动或这些信息将能够被再次使用，Weblogic Server支持的持久性方 式包括文件、数据库、客户端cookie保存和复制。复制严格说来不算持久化保存，因为session实际上还是保存在内存里，不过同样的信息 被复制到各个cluster内的服务器进程中，这样即使某个服务器进程停止工作也仍然可以从其 他进程中取得session。 6、socket编程中，客户端和服务端建立连接需要用到哪些函数并解释一下作用。 7、socket连接中shutdown()和close()有什么区别。 close关闭的是双向的，in out都关了~~但是是引用计数--直到--到为0才真正的关闭~因而是线程安全的。 shutdown 是单向关闭，通过参数指定关闭不同的方向，直接强制关闭，非线程安全； 8、多线程和多进程的区别和联系，多进程访问临界资源如何处理。 进程是资源分配的最小单位，线程是CPU调度的最小单位 多线程之间共享同一 个进程的地址空间，线程间通信简单，同步复杂，线程创建、销毁和切换简单，速度快，占用内存少，适用于多核分布式系统，但是线程间会相互影响，一个线程意 外终止会导致同一个进程的其他线程也终止，程序可靠性弱。 多进程间拥有各自 独立的运行地址空间，进程间不会相互影响，程序可靠性强，但是进程创建、销毁 和切换复杂，速度慢，占用内存多，进程间通信复杂，但是同步简单，适用于多核 、多机分布。 8、手撕算法题，输出某二叉树中路径和等于定值n的所有路径，路径：从根节点到叶结点或从叶节点到叶节点 想法：将二叉树建立子节点指向父节点的引用，BFS遍历某叶节点或root结点到剩下结点的的路径，和为n则输出。 9、手撕代码，给出二叉树中的两个结点，找出它们的最低公共父节点。 BFS或DFS找到指点两个结点n1和n2，记录root结点到n1和n2结点的路径，最后一个重复结点即为最低公共父结点。 二面： 一面面经写到一半，然后打电话来了二面，效率是真的高。二面小哥超帅的，总共面了47分钟，大多时间都在撕代码，或者在撕代码的路上。 自我介绍，然后讲3分钟项目，根据项目提了一些对应的问题，然后就是你问我答环节。 1、kafka当分区数大于消费者数量的时候如何消费，反过来呢？ 分区数&gt;消费者数：一个消费者消费固定负责一个或多个分区数据； 分区数&lt;消费者数：一个消费者固定消费一个分区，多余消费者空闲。 2、kafka如何保证多分区数据的顺序性。 采用重排序、阻塞、单分区。 3、kakfa在消费者端调试过哪些参数，有什么意义；在使用poll拉取消息的时候有个除了有个每次拉取的数据条数设置还有哪些参数。 // 创建一份kafka参数map Map&lt;String, Object&gt; kafkaParams = new HashMap&lt;String, Object&gt;(); kafkaParams.put(&quot;bootstrap.servers&quot;, &quot;master:9092,slave1:9092,slave2:9092&quot;); kafkaParams.put(&quot;key.deserializer&quot;, StringDeserializer.class); kafkaParams.put(&quot;value.deserializer&quot;, ByteArrayDeserializer.class); kafkaParams.put(&quot;group.id&quot;, &quot;topic6-groud1&quot;); kafkaParams.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;); kafkaParams.put(&quot;enable.auto.commit&quot;, true); Collection&lt;String&gt; topics = Arrays.asList(&quot;topic6&quot;); while (true) { ConsumerRecords&lt;String, byte[]&gt; records = consumer.poll(Duration.ofMillis(500)); for (ConsumerRecord&lt;String, byte[]&gt; record : records) { } } 4、介绍一下spark流式计算框架，RDD弹性数据集以及对其的了解。 5、hbase优化策略调过哪些参数。 6、多线程与多进程的区别（两面都问到过），在线程和进程切换时有什么区别，哪个开销大，为什么？ 7、排序算法中有哪些是稳定的，哪些不稳定 稳定算法：冒泡排序、插入排序、归并排序、基数排序 不稳定算法：选择排序、快速排序、希尔排序、堆排序 8、linux命令，查看内存情况，剩余磁盘空间，网络状态 top、df -lh、netstat 9、构造最小生成树Prim算法（不记得） 10、迪杰斯特拉算法求最短路径（还是不记得，默默心疼一波自己） 11、手撕一个算法题 有多个柱子靠在一起，柱子宽度固定为1，给定一个数组，表示每个柱子的高度，问从柱子上方泼水，柱子中最多能容纳多少水。示意图如下 首先想到的是计算每个柱子相邻的两个柱子较低的那个高度，然后相加。在小哥的提醒下发现，当如图中第7根和第9根酱紫的时候不对。 然后我又想了一会儿，提出对每个柱子遍历，找到其左右两边的最高的那个柱子，取较低的那个然后所有相加。然后小哥说复杂度太大了，有没有办法在O(n)的复杂度内把左右两边的最高柱子算出来。 然后我想了会儿，采用遍历一次，每次用tempL和tempR记录当前最大值，放进辅助数组里，然后让我用代码写一下。然后在作业本上撕了一下： 答案还在手打中，待续。。。 ","link":"https://kily-007.github.io/post/瓜子二手车面经-大数据岗/"},{"title":"集群搭建记录","content":"2018/12/12整理 Centos7 jdk1.8.0 hadoop-2.8.4 zookeeper-3.5.4 hbase-2.0.1 flume1.8.0 spark-2.3.1-bin-hadoop2.7 scala-2.12.6 redis-4.0.11 校准系统时间：sudo /usr/sbin/ntpdate stdtime.gov.hk 根据矫正过的系统时间设置硬件时钟：sudo hwclock -w 目录 1、安装hadoop-2.8.4 1 2、配置zookeeper-3.5.4-beta 4 3、安装hbase 5 4、hbase建库建表 7 5、安装flume 8 6、安装spark 9 7、安装redis-4.0.11 10 8、kafka集群安装配置 12 9、spark streaming 接收kafka通道数据流 1、安装hadoop-2.8.4 1、修改主机名 机名修改vim /etc/hostname 映射修改vim /etc/hosts 192.168.1.100 master 192.168.1.101 slave1 192.168.1.102 slave2 检查是否ping通：ping master –c 3 2、创建hadoop用户和用户组 添加用户：useradd hadoop 密码：passwd hadoop 将用户添加到组：usermod –a –G hadoop hadoop 查询结果：cat /etc/group 设置管理员权限：visudo 修改文件如下: root ALL=(ALL) ALL hadoop ALL=(ALL) ALL 2、设置SSH免密登录 用hadoop用户登录 #查看是否安装了ssh服务：rpm –qa | grep ssh 查看ssh服务是否启动：ps -e | grep ssh 测试SSH是否能用：ssh 主机名 cd ~/.ssh/ 创建钥匙：ssh-keygen -t rsa 将公匙加入keys中：cat id_rsa.pub &gt;&gt; authorized_keys 修改文件夹权限chmod 600 ./authorized_keys 把node1和node2的id_rsa.pub传给master: scp id_rsa.pub admin@master:~/.ssh/id_rsa.pub.slave1 在master上综合所有公匙： cat id_rsa.pub&gt;&gt;authorized_keys cat id_rsa.pub.slave1&gt;&gt;authorized_keys cat id_rsa.pub.slave2&gt;&gt;authorized_keys 将master中的公匙authorized_keys复制到node1和node2的.ssh目录下 scp authorized_keys admin@slave1:/home/admin/.ssh/authorized_keys scp authorized_keys admin@slave2:/home/admin/.ssh/authorized_keys 修改屏幕分辨率: 1 查看现有分辨率 xrandr 2 添加自己想要的分辨率 cvt 1600 900 3 增加新的分辨率 xrandr --newmode &quot;16000x900_60.00&quot; 106.50 1440 1528 1672 1904 900 903 909 934 -hsync +vsync --newmode后面的参数就是使用cvt的返回值Modeline后面的部分 4.为当前显示增加这个分辨率 xrandr --addmode VGA-1 1440x900_60.00 5 设置分辨率 xrandr --output VGA-1 --mode 1600x900_60.00 3、安装jdk 查看原装jdk： rpm -qa | grep java 卸载原先的jdk： sudo rpm -e --nodeps java-1.8.0-openjdk-1.8.0.181-7.b13.el7.x86_64 sudo rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.181-7.b13.el7.x86_64 sudo rpm -e --nodeps java-1.7.0-openjdk-1.7.0.171-2.6.15.5.el7.x86_64 sudo rpm -e --nodeps java-1.7.0-openjdk-headless-1.7.0.191-2.6.15.5.el7.x86_64 sudo mkdir /usr/java 解压到/sur/java中：sudo tar -zxvf 下载/jdk-8u172-linux-x64.tar.gz -C /usr/java/ 配置本用户的环境变量：sudo vim /etc/profile export JAVA_HOME=/usr/java/jdk1.8.0 export CLASSPATH=.:JAVAHOME/lib/dt.jar:JAVA_HOME/lib/dt.jar:JAVAH​OME/lib/dt.jar:JAVA_HOME/lib/tools.jar export PATH=JAVAHOME/bin:JAVA_HOME/bin:JAVAH​OME/bin:PATH 4、用hadoop用户进入安装hadoop 解压hadoop：tar -zxvf 下载/hadoop-2.8.4.tar.gz -C /home/admin/app/ sudo vim /home/admin/app/hadoop/etc/hadoop/hadoop-env.sh 找到JAVA_HOME变量，修改此变量如下 export JAVA_HOME=/usr/java/jdk1.8.0 赋予读写权限：sudo chmod 774 /home/admin/app/hadoop/etc/hadoop/ 修改配置文件：vim /home/admin/app/hadoop/etc/hadoop/core-site.xml ①core-site.xml hadoop.tmp.dir file:/home/admin/app/hadoop/tmp fs.defaultFS hdfs://master:9000 ②hdfs-site.xml dfs.replication 2 dfs.namenode.name.dir file:/home/admin/app/hadoop/tmp/dfs/name dfs.datanode.data.dir file:/home/admin/app/hadoop/tmp/dfs/data dfs.datanode.http.address 0.0.0.0:50075 dfs.replication.interval 30 dfs.datanode.handler.count 300 dfs.namenode.handler.count 300 dfs.datanode.max.xcievers 1024 ③mapred-site.xml mapreduce.framework.name yarn mapreduce.jobhistory.address master:10020 mapreduce.jobhistory.webapp.address master:19888 ④yarn-site.xml yarn.resourcemanager.hostname master yarn.nodemanager.aux-services mapreduce_shuffle yarn.log-aggregation-enable true ⑤在master机上在slaves中将localhost改为node1和node2 修改hadoop启动与停止pid文件 将hadoop/sbin/hadoop-daemon.sh中添加HADOOP_PID_DIR=/home/admin/app/hadoop/pids 将hadoop/sbin/yarn-daemon.sh中添加TARN_PID_DIR=/home/admin/app/hadoop/pids ⑥时间同步: 在hadoop-env.sh添加: export HADOOP_OPTS=&quot;HADOOPOPTS−Duser.timezone=GMT+08&quot;在yarn−env.shYARNOPTS=&quot;HADOOP_OPTS -Duser.timezone=GMT+08&quot; 在yarn-env.sh YARN_OPTS=&quot;HADOOPO​PTS−Duser.timezone=GMT+08&quot;在yarn−env.shYARNO​PTS=&quot;YARN_OPTS -Duser.timezone=GMT+08&quot; ⑦发送到slave1,slave2 scp -r hadoop admin@slave1:/home/admin/app/ 首次启动格式化：hdfs namenode -format 在浏览器中 http://master:8088 master:50070 hadoop进程： master: NameNode,SecondaryNameNode,ResourceManager slave1:DataNode,NodeManager slave2:DataNode,NodeManager 5、 192.168.146.140 master 192.168.146.141 node1 192.168.146.142 node2 关闭防火墙：systemctl stop firewalld 查看防火墙状态：firewall-cmd --state 2、配置zookeeper-3.5.4-beta 1解压：cd /home /下载 tar -zxvf 下载/zookeeper-3.5.4-beta.tar.gz -C /home/admin/app 2配置环境变量： vim /etc/profile source /etc/profile export ZOOKEEPER_HOME=/home/admin/app/zookeeper export PATH=PATH:PATH:PATH:ZOOKEEPER_HOME/bin:$ZOOKEEPER_HOME/conf 3修改配置文件 cd /home/hadoop/zookeeper-3.5.4-beta/conf cp zoo_sample.cfg zoo.cfg vim zoo.cfg tickTime=5000 initLimit=10 syncLimit=5 dataDir=/home/hadoop/app/zookeeper/data clientPort=2181 dataLogDir=/home/hadoop/app/zookeeper/datalog server.1=master:2888:3888 server.2=node1:2888:3888 server.3=node2:2888:3888 在zookeeper文件夹下创建data和datalog文件夹，在data文件夹下创建一个文件myid，内容为数字x，x为上配置中server.x中的数字： vim /home/hadoop//zookeeper-3.5.4-beta/data/myid 4启动zookeeper：注意要按照myid大小顺序来依次在主机上启动: zkServer.sh start zkServer.sh restart zkServer.sh stop 启动zookeeper/bin/zkServer.sh start 重启zookeeper/bin/zkServer.sh restart 查看状态zookeeper/bin/zkServer.sh status 修改文件夹权限：sudo chmod -R 777 zookeeper 修改文件及子文件所属用户和用户组：sudo chown -R hadoop:hadoop zookeeper 启动成功：输入zkServer.sh status命令可以看到如下信息则表示安装配置成功 Mode:follower(或者是leader) zookeeper进程: master:QuorumPeerMain slave1:QuorumPeerMain slave2:QuorumPeerMain 连接客户端:zkCli.sh -timeout 5000 -r -server master:2181 3、安装hbase 1解压：cd /下载 tar -zxvf 下载/hbase-2.0.1-bin.tar.gz -C /home/admin/app 2配置环境变量： vim /etc/profile source /etc/profile export HBASE_HOME=/home/admin/app/hbase export PATH=PATH:PATH:PATH:HBASE_HOME/bin 3配置文件sudo gedit hbase/conf/hbase-env.sh hbase-env.sh export JAVA_HOME=/usr/java/jdk1.8.0 export HBASE_MANAGES_ZK=false hbase-site.xml hbase.rootdir hdfs://master:9000/hbase hbase.cluster.distributed true hbase.zookeeper.quorum master,slave1,slave2 hbase.master master:60000 regionservers slave1 slave2 将hbase文件发送到slave1和slave2的相应位置: scp -r hbase admin@slave1:/home/admin/app/ scp -r hbase admin@slave2:/home/admin/app/ 4启动hbase: start-hbase.sh 启动HBase的shell命令：hbase shell 先启动regionserver，在启动HMaster 在regionServer上./hbase-daemon.sh start regionserver 在master上执行：./bin/hbase-daemon.sh start master 5访问web页面:http://master:16010 6.设置备用hmaster: 在hbase/conf下新建文件backup-masters ,添加slave1作为备用hmaster 7hbase启动错:Causedby:java.lang.ClassNotFoundException:org.apache.htrace.SamplerBuilder 解决办法:cp $HBASE_HOME/lib/client-facing-thirdparty/htrace-core-3.1.0-incubating.jar $HBASE_HOME/lib/ hbase进程： master: HMaster slave1:HRegionServer slave2:HRegionServer 4、hbase建库建表 ① 建表： create ‘stu’, ‘information’ ,‘score’ 禁用表：disable ‘stu’ 删除表：（删除之前必须先禁用）：drop 'stu' 验证表是否被删除: exists 'stu' 清空表:truncate 'test' create 'test','ts',SPLITS=&gt;['0050','0100','0150','0200','0250','0300','0350','0400','0450','0500','0550','0600','0650','0700','0750','0800','0850','0900','0950','1000'] create 'test','ts',SPLITS=&gt;['10','20','30'] ②增 插值： put 'stu','row0','information:name','Chengang' put 'stu','row0','information:sex','man' put 'stu','row0','information:age','23' put 'stu','row0','score:math','99' put 'stu','row0','score:English','89' put 'stu','row0','score:hbase','100' ③删 删除特定单元格：delete ‘stu’,’row2’,’score:hbase’ 删除所有单元格：delete ‘stu’,’row2’ ④查 读取指定行：get ’’,’row1’ 读取指定列：get 'stu','row1',{COLUMN=&gt;'information:name'} ⑤改 更新：put 'stu','row2','score:English','99' ⑥常用指令： 1列出表中所有数据：scan ‘stu’ 2计算表的行数量：count ‘stu’ 3为用户hadoop授予特定的权限:grant ‘hadoop’,’RXA’ R - 代表读取权限 W - 代表写权限 X - 代表执行权限 C - 代表创建权限 A - 代表管理权限 4撤销用户hadoop访问表(所有权限)的权限：revoke ‘hadoop’ 5列出stu表的所有用户权限: user_permission ‘stu’ 5、安装flume 1、解压 安装 sudo tar -zxvf apache-flume-1.8.0-bin.tar.gz -C /home/hadoop 2、配置环境变量 sudo gedit /etc/profile source /etc/profile export FLUME_HOME=/home/hadoop/flume export FLUME_CONF=FLUMEHOME/confexportPATH=FLUME_HOME/conf export PATH=FLUMEH​OME/confexportPATH=PATH:$FLUME_HOME 3、配置flume-env.sh cd /home/hadoop/flume/conf sudo cp ./flume-env.sh.template ./flume-env.sh sudo gedit ./flume-env.sh export JAVA_HOME=/usr/java/jdk1.8.0 4、 写hbase-sink-aysn.conf,用于执行flume写入hbase gedit /home/hadoop/flume/conf/hbase-sink-aysn.conf 将以下复制： Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 Describe/configure the source a1.sources.r1.type = syslogtcp a1.sources.r1.bind = master a1.sources.r1.port = 44444 a1.sources.r1.eventSize = 1000 Describe the sink a1.sinks.k1.type = asynchbase a1.sinks.k1.table = student a1.sinks.k1.columnFamily = information a1.sinks.k1.serializer.payloadColumn=id,name,age, a1.sinks.k1.batchSize = 10 a1.sinks.k1.serializer = org.apache.flume.sink.hbase.AsyncHbaseEventSerializer a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 5、启动flume服务 flume-ng agent -c conf -f tcp_hbase.conf -n a1 -Dflume.root.logger=INFO,console --conf = conf 查看写入的数据文件hadoop fs -ls /flume 打开数据文件hadoop fs -cat /flume/master.1531133606859 安装spark 安装spark之前需要先安装scala 1解压到/home/hadoop/app : tar -zxvf scala-2.12.6.tgz -C /home/admin/app 将 scala-2.12.6重命名为scala 2配置环境变量: export SCALA_HOME=/home/admin/app/scala export PATH=PATH:PATH:PATH:SCALA_HOME/bin 3测试是否安装成功:scala -version 安装spark 1 解压到/home/hadoop/app : tar -zxvf spark-2.3.1-bin-hadoop2.7.tgz -C /home/admin/app 将 spark-2.3.1-bin-hadoop2.7重命名为spark 2 配置环境变量: export SPARK_HOME=/home/admin/app/spark export PATH=PATH:PATH:PATH:SPARK_HOME/sbin:SPARKHOME/bin3配置/spark/conf/spark−env.sh文件exportJAVAHOME=/usr/local/jdk1.8.0exportHADOOPHOME=/home/hadoop/app/hadoopexportSCALAHOME=/home/hadoop/app/scalaexportHBASEHOME=/home/hadoop/app/hbaseexportHADOOPCONFDIR=/home/hadoop/app/hadoop/etc/hadoopexportSPARKMASTERHOST=masterexportSPARKMASTERPORT=7077exportSPARKMASTERWEBUIPORT=7078exportSPARKWORKERMEMORY=30GexportLDLIBRARYPATH=SPARK_HOME/bin 3 配置/spark/conf/spark-env.sh文件 export JAVA_HOME=/usr/local/jdk1.8.0 export HADOOP_HOME=/home/hadoop/app/hadoop export SCALA_HOME=/home/hadoop/app/scala export HBASE_HOME=/home/hadoop/app/hbase export HADOOP_CONF_DIR=/home/hadoop/app/hadoop/etc/hadoop export SPARK_MASTER_HOST=master export SPARK_MASTER_PORT=7077 export SPARK_MASTER_WEBUI_PORT=7078 export SPARK_WORKER_MEMORY=30G export LD_LIBRARY_PATH=SPARKH​OME/bin3配置/spark/conf/spark−env.sh文件exportJAVAH​OME=/usr/local/jdk1.8.0exportHADOOPH​OME=/home/hadoop/app/hadoopexportSCALAH​OME=/home/hadoop/app/scalaexportHBASEH​OME=/home/hadoop/app/hbaseexportHADOOPC​ONFD​IR=/home/hadoop/app/hadoop/etc/hadoopexportSPARKM​ASTERH​OST=masterexportSPARKM​ASTERP​ORT=7077exportSPARKM​ASTERW​EBUIP​ORT=7078exportSPARKW​ORKERM​EMORY=30GexportLDL​IBRARYP​ATH=HADOOP_HOME/lib/native 4 cp spark-env.sh.template spark-env.sh 加入spark.master spark://master:8040 5 cp slaves.template slaves 加入 master slave1 slave2 6 启动spark集群: start-all.sh stop-all.sh 7.spark提交任务 ./bin/spark-submit --class com.spark.kafkaConsumer.SparkFromKafka --master spark://master:7077 --executor-memory 8G --total-executor-cores 8 /home/hadoop/workspace/KafkaConsumer/target/KafkaConsumer-0.0.1-SNAPSHOT-jar-with-dependencies.jar spark-shell --master yarn nmap -p 7077 master_ip 7.安装redis-4.0.11 192.168.1.100 7001 7004 192.168.1.101 7002 7005 192.168.1.102 7003 7006 1.解压到/home/hadoop/app :tar -zxvf redis-4.0.11.tar.gz -C /home/admin/app 重命名为redis 编译:make &amp;&amp; make install 2.配置redis集群： 在redis/目录下： mkdir -p redis-cluster/{7001,7004} cp redis.conf redis-cluster/7001/redis-7001.conf cp redis.conf redis-cluster/7004/redis-7004.conf 修改： bind 192.168.1.100 port 7001 daemonize yes （redis后台运行） cluster-enabled yes（启动集群模式） appendonly yes (开启自动持久化) dir /home/admin/app/redis/redis-cluster/7001/data (data存放目录) logfile /home/admin/app/redis/redis-cluster/7001/log/redis-7001.log pidfile /var/run/redis_7001.pid （pid 7001和port要对应） cluster-config-file nodes-7001.conf cluster-node-timeout 15000 3.指定配置文件启动: cd redis/src ./redis-server ../reids.conf 5.集群搭建 每台机子上都按上述步骤安装redis 6.配置步骤参照https://www.cnblogs.com/ding2016/p/7892542.html 6.2删除某文件夹下所有文件: rm -r app/redis/redis-cluster/7002/data/* rm -r app/redis/redis-cluster/7002/log/* 7.启动集群: redis/ 启动六个节点的服务: master: src/redis-server redis-cluster/7001/redis-7001.conf app/redis/src/redis-server app/redis/redis-cluster/7004/redis-7004.conf slave1: app/redis/src/redis-server app/redis/redis-cluster/7002/redis-7002.conf app/redis/src/redis-server app/redis/redis-cluster/7005/redis-7005.conf slave2: app/redis/src/redis-server app/redis/redis-cluster/7003/redis-7003.conf app/redis/src/redis-server app/redis/redis-cluster/7006/redis-7006.conf 查看服务是否启动: ps -ef | grep redis 进入redis目录启动集群 ; src/redis-trib.rb create --replicas 1 192.168.1.100:7001 192.168.1.101:7002 192.168.1.102:7003 192.168.1.100:7004 192.168.1.101:7005 192.168.1.102:7006 src/redis-trib.rb create --replicas 1 110.1.1.1:7001 110.1.1.101:7002 110.1.1.102:7003 110.1.1.1:7004 110.1.1.101:7005 110.1.1.102:7006 进入客户端: ./src/redis-cli -c -h 192.168.1.100 -p 7001 查看redis的延迟数据: Redis-cli --latency -h 127.0.0.1 -p 6379 7.5重启后报错:Node 192.168.1.101:7001 is not empty. Either the node already knows other no ps -ef | grep redis , kill 掉6个redis进程,删除reids/redis-cluster/7001/data中的所有文件 重新启动所有服务 新增结点:src/redis-trib.rb add-node 192.168.1.100:7007 192.168.1.100:7001 8.常见redis命令 集群 cluster info 打印集群的信息 cluster nodes 列出集群当前已知的所有节点（node），以及这些节点的相关信息。 节点 cluster meet 将ip和port所指定的节点添加到集群当中，让它成为集群的一份子。 cluster forget &lt;node_id&gt; 从集群中移除 node_id 指定的节点。 cluster replicate &lt;node_id&gt; 将当前节点设置为node_id指定的节点的从节点。 cluster saveconfig 将节点的配置文件保存到硬盘里面。 槽(slot) cluster addslots [slot ...] 将一个或多个槽（slot）指派（assign）给当前节点。 cluster delslots [slot ...] 移除一个或多个槽对当前节点的指派。 cluster flushslots 移除指派给当前节点的所有槽，让当前节点变成一个没有指派任何槽的节点。 cluster setslot node &lt;node_id&gt; 将槽 slot 指派给 node_id 指定的节点，如果槽已经指派给另一个节点，那么先让另一个节点删除该槽&gt;，然后再进行指派。 cluster setslot migrating &lt;node_id&gt; 将本节点的槽 slot 迁移到 node_id 指定的节点中。 cluster setslot importing &lt;node_id&gt; 从 node_id 指定的节点中导入槽 slot 到本节点。 cluster setslot stable 取消对槽 slot 的导入（import）或者迁移（migrate）。 键 cluster keyslot 计算键 key 应该被放置在哪个槽上。 cluster countkeysinslot 返回槽 slot 目前包含的键值对数量。 cluster getkeysinslot 返回 count 个 slot 槽中的键。 redis数据备份及回复 dump.rdb 方式 安装数据解析工具执行命令: easy_install pip install rdbtools 将rdb文件转为json文件 rdb --command json redis-cluster/7001/data/dump.rdb&gt;redis-cluster/7001/data/dump.json 8 .kafka集群安装配置 ①解压到指定文件夹 tar -zxvf 下载/kafka_2.12-2.0.0.tgz -C /home/admin/app/ ②修改配置文件 kafka/config/server.properties broker.id=0 listeners=PLAINTEXT://master:9092 advertised.listeners=PLAINTEXT://master:9092 log.dirs=/home/admin/app/kafka/kafka-logs zookeeper.connect=master:2181,slave1:2181,slave2:2181 ③讲kafka文件发送到slave1和slave2 scp -r kafka admin@slave1:/home/admin/app/ scp -r kafka admin@slave2:/home/admin/app/ ④修改slave1中的配置文件kafka/config/server.properties broker.id=1 listeners=PLAINTEXT://slave1:9092 advertised.listeners=PLAINTEXT://slave1:9092 log.dirs=/home/hadoop/app/kafka/kafka-logs zookeeper.connect=master:2181,slave1:2181,slave2:2181 ⑤修改slave2中的配置文件kafka/config/server.properties broker.id=2 listeners=PLAINTEXT://slave2: advertised.listeners=PLAINTEXT://slave2:9092 log.dirs=/home/hadoop/app/kafka/kafka-logs zookeeper.connect=master:2181,slave1:2181,slave2:2181 ⑥测试 在每个节点启动kafka服务: bin/kafka-server-start.sh -daemon config/server.properties &amp; 后台启动:bin/kafka-server-start.sh config/server.properties 1&gt;/dev/null 2&gt;&amp;1 &amp; 1创建topic: ./bin/kafka-topics.sh --create --zookeeper master:2181,slave1:2181,slave2:2181 --replication-factor 2 --partitions 1 --topic topic1 2生产者bin/kafka-console-producer.sh --broker-list slave1:9092 --topic topic6 3消费者bin/kafka-console-consumer.sh --bootstrap-server master:9092 --topic topic6 –from-beginning 4查看所有topic: ./bin/kafka-topics.sh --list --zookeeper master:2181 5查看某topic详细信息: bin/kafka-topics.sh --describe --zookeeper master:2181 --topic topic1 6 查看所有topic详细信息: bin/kafka-topics.sh --describe --zookeeper master:2181 7删除topic :bin/kafka-topics.sh --delete --zookeeper master:2181 --topic topic1 8停止kafka服务:bin/kafka-server-stop.sh 9错误记录: kafka进程启动后一会儿自动挂掉,修改/kafka/kafka-logs/meta.properties中的broker.id与server.properties中的broker.id一致 10错误记录: KafkaConsumer is not safe for multi-threaded access KafkaConsumer是非线程安全的类，当使用多个线程操作同一个KafkaConsumer对象时就会引起这个错误。 PartitionCount：分区个数 ??ReplicationFactor：副本个数 ??Partition：分区 ??Leader：负责处理消息的读和写，leader是从所有节点中随机选择的.当前partition起作用的breaker.id ??Replicas: 列出了所有的副本节点，不管节点是否在服务中. ??Isr：是正在服务中的节点. 9.spark streaming 接收kafka通道数据流 ②集群 ./bin/spark-submit --class edu.wuli.gqzx.kafkaConsumer.SparkFromKafka --master spark://master:7077 --executor-memory 8G --total-executor-cores 30 /home/admin/workspace/workspace2/KafkaConsumer/target/KafkaConsumer-0.0.1-SNAPSHOT-jar-with-dependencies.jar 10.spark数据处理记录 ①map函数与flatmap函数 map函数: 顾名思义，将一个函数传入map中，然后利用传入的这个函数，将集合中的每个元素处理，并将处理后的结果返回。 flatMap函数: flatMap与map唯一不一样的地方就是传入的函数在处理完后返回值必须是List，其实这也不难理解，既然是flatMap，那除了map以外必然还有flat的操作，所以需要返回值是List才能执行flat这一步. 11.redis优化记录 redis-7001.conf文件: maxmemory 68719476736 #559 maxmemory-policy volatile-ttl #590 cluster-require-full-coverage no #905 1） 在Redis配置文件中(一般叫Redis.conf)，通过设置“maxmemory”属性的值可以限制Redis最大使用的内存若是启用了Redis快照功能，应该设置“maxmemory”值为系统可使用内存的45%，因为快照时需要一倍的内存来复制整个数据集，也就是说如果当前已使用45%，在快照期间会变成95%(45%+45%+5%)，其中5%是预留给其他的开销。 如果没开启快照功能，maxmemory最高能设置为系统可用内存的95%。 2) 当内存使用达到设置的最大阀值时，需要选择一种key的回收策略，可在Redis.conf配置文件中修改“maxmemory-policy”属性值。 若是Redis数据集中的key都设置了过期时间，那么“volatile-ttl”策略是比较好的选择。但如果key在达到最大内存限制时没能够迅速过期，或者根本没有设置过期时间。那么设置为“allkeys-lru”值比较合适，它允许Redis从整个数据集中挑选最近最少使用的key进行删除(LRU淘汰算法)。Redis还提供了一些其他淘汰策略，如下： volatile-lru：使用LRU算法从已设置过期时间的数据集合中淘汰数据。 volatile-ttl：从已设置过期时间的数据集合中挑选即将过期的数据淘汰。 volatile-random：从已设置过期时间的数据集合中随机挑选数据淘汰。 allkeys-lru：使用LRU算法从所有数据集合中淘汰数据。 allkeys-random：从数据集合中任意选择数据淘汰 no-enviction：禁止淘汰数据。 3） 槽是否全覆盖：cluster-require-full-coverage no。默认是yes，只要有结点宕机导致16384个槽没全被覆盖，整个集群就全部停止服务，所以一定要改为no 4） list-compress-depth 0 设置: 列表也可能被压缩 0:禁用所有列表压缩。 1:depth 1的意思是“在1个节点进入列表后，从头部或尾部开始压缩” So:[head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]，[head],[tail]将始终未压缩；内部节点将压缩。 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail] 2在这里的意思是:不要压缩head或head-&gt;next或tail-&gt;prev或tail，而是压缩它们之间的所有节点。 3:[head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail] 5) 内核参数 overcommit_memory 1 有三种方式修改内核参数，但要有root权限： （1）编辑/etc/sysctl.conf ，改vm.overcommit_memory=1，然后sysctl -p 使配置文件生效 （2）sysctl vm.overcommit_memory=1 （3）echo 1 &gt; /proc/sys/vm/overcommit_memory 建议使用方法 1 它是 内存分配策略 可选值：0、1、2。 0， 表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存，内存申请允许；否则，内存申请失败，并把错误返回给应用进程。 1， 表示内核允许分配所有的物理内存，而不管当前的内存状态如何。 2， 表示内核允许分配超过所有物理内存和交换空间总和的内存 hbase优化记录 putlist大小 2524288000/74176 2524288000/60608 hbase.ipc.server.callqueue.handler.factor 指定调用队列的数量： 0 意味着单个共享队列。 1 意味着每个处理程序的一个队列。 一个0和1之间的值，按处理程序的数量成比例地分配队列数。例如，0.5 的值在每个处理程序之间共享一个队列。 2)使用 hbase.ipc.server.callqueue.read.ratio（hbase.ipc.server.callqueue.read.share在0.98中）将调用队列分成读写队列： 0.5 意味着将有相同数量的读写队列。 &lt;0.5 表示为读多于写。 0.5 表示写多于读 3)禁用 Nagle 的算法。延迟的 ACKs 可以增加到200毫秒的 RPC 往返时间。设置以下参数： 在 Hadoop 的 core-site.xml 中： ipc.server.tcpnodelay = true ipc.client.tcpnodelay = true 在 HBase 的 hbase-site.xml 中： hbase.ipc.client.tcpnodelay = true hbase.ipc.server.tcpnodelay = true 操作系统优化 1)修改linux打开文件数和进程数 sudo vim /etc/security/limits.conf 加入: soft nofile 102400 hard nofile 409600 sudo vim /etc/security/limits.d/20-nproc.conf soft nproc 409600 hard nproc 819200 2）禁用透明大页面压缩 root用户执行 echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag 开机禁用: sudo vim /etc/rc.local echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag ","link":"https://kily-007.github.io/post/笔记-集群搭建记录/"},{"title":"笔记-java中的锁","content":"笔记-java中的锁 java对象 java对象=对象头+实例数据+对其填充 java对象头 对象头=MarkWord+class pointer+array length(若对象为数组) 锁存在Java对象头里。如果对象是数组类型，则虚拟机用3个Word（字宽）存储对象头，如果对象是非数组类型，则用2字宽存储对象头。在32位虚拟机中，一字宽等于四字节，即32bit。 //普通对象： |-------------------------------- -------------------------| | Object Header (64 bits) | |--------------------------------|-------------------------| | Mark Word (32 bits) | Klass Word (32 bits) | |--------------------------------|-------------------------| //数组对象： |---------------------------------------------------------------------------------| | Object Header (96 bits) | |--------------------------------|-----------------------|------------------------| | Mark Word(32bits) | Klass Word(32bits) | array length(32bits) | |--------------------------------|-----------------------|------------------------| 对象头的组成 Mark Word 32位JVM的Mark Word默认存储结构如下 class pointer 这一部分用于存储对象的类型指针，该指针指向它的类元数据，JVM通过这个指针确定对象是哪个类的实例。 array length 如果对象是一个数组，那么对象头还需要有额外的空间用于存储数组的长度。 总结 长度 内容 说明 32/64bit Mark Word 存储对象的hashCode或锁信息等 32/64bit Class Metadata Address 存储到对象类型数据的指针 32/64bit Array length 数组的长度（如果当前对象是数组） 自旋锁 自旋锁原理：如果持有锁的线程能在很短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞挂起状态，它们只需要等一等（自旋），等持有锁的线程释放锁后即可立即获取锁，这样就避免用户线程和内核的切换的消耗。但是线程自旋是需要消耗cup的，说白了就是让cup在做无用功，线程不能一直占用cup自旋做无用功，所以需要设定一个自旋等待的最大时间。如果持有锁的线程执行的时间超过自旋等待的最大时间扔没有释放锁，就会导致其它争用锁的线程在最大等待时间内还是获取不到锁，这时争用线程会停止自旋进入阻塞状态。 自选时间阈值：自旋锁的目的是为了占着CPU的资源不释放，等到获取到锁立即进行处理。但是如何去选择自旋的执行时间呢？如果自旋执行时间太长，会有大量的线程处于自旋状态占用CPU资源，进而会影响整体系统的性能。因此自旋的周期选的额外重要！ JVM对于自旋周期的选择，jdk1.5这个限度是一定的写死的，在1.6引入了适应性自旋锁，适应性自旋锁意味着自旋的时间不在是固定的了，而是由前一次在同一个锁上的自旋时间以及锁的拥有者的状态来决定，基本认为一个线程上下文切换的时间是最佳的一个时间，同时JVM还针对当前CPU的负荷情况做了较多的优化 如果平均负载小于CPUs则一直自旋 如果有超过(CPUs/2)个线程正在自旋，则后来线程直接阻塞 如果正在自旋的线程发现Owner发生了变化则延迟自旋时间（自旋计数）或进入阻塞 如果CPU处于节电模式则停止自旋 自旋时间的最坏情况是CPU的存储延迟（CPU A存储了一个数据，到CPU B得知这个数据直接的时间差） 自旋时会适当放弃线程优先级之间的差异 偏向锁 偏向锁:简单来说就是一段同步代码块一直被一个线程所访问，那么该线程就会自动获取锁，降低获取锁的代价。 它会偏向最先获得它的线程，当一个线程访问同步代码块获得锁时，会在对象头和栈帧记录里存储锁偏向的线程ID，当这个线程再次进入同步代码块时，就不需要CAS操作来加锁了，只要测试一下对象头里是否存储着指向当前线程的偏向锁。如果测试成功，则表明该线程已经获得了锁，如果失败，则减产偏向锁的标示是否设为1，也就是当前是否是偏向锁，如果是，则尝试用CAS操作将对象头的偏向锁指向当前线程，如果不是，则用CAS竞争锁。 撤销: 当有其他线程竞争偏向锁的时候，持有偏向锁的线程才会释放锁。当前持有偏向锁的线程如果处于不活动的状态，则直接将对象头设置为无锁状态；如果线程仍然活动，拥有偏向锁的栈会被执行，然后将锁偏向于其他线程，或者恢复到无锁状态。 在没有实际竞争的情况下，还能够针对部分场景继续优化。如果不仅仅没有实际竞争，自始至终，使用锁的线程都只有一个，那么，维护轻量级锁都是浪费的。偏向锁的目标是，减少无竞争且只有一个线程使用锁的情况下，使用轻量级锁产生的性能消耗。轻量级锁每次申请、释放锁都至少需要一次CAS，但偏向锁只有初始化时需要一次CAS。 轻量级锁 轻量级锁是由偏向所升级来的，偏向锁运行在一个线程进入同步块的情况下，当第二个线程加入锁争用的时候，偏向锁就会升级为轻量级锁。其它线程会通过自选的形式尝试获取锁，不会阻塞，提高性能。 **优点：**轻量级锁尽可能的减少线程的阻塞，这对于锁的竞争不激烈，且占用锁时间非常短的代码块来说性能能大幅度的提升，因为自旋的消耗会小于线程阻塞挂起再唤醒的操作的消耗，这些操作会导致线程发生两次上下文切换！ **缺点：**但是如果锁的竞争激烈，或者持有锁的线程需要长时间占用锁执行同步块，这时候就不适合使用轻量级锁了，因为轻量级锁在获取锁前一直都是占用cpu做无用功，同时有大量线程在竞争一个锁，会导致获取锁的时间很长，线程自旋的消耗大于线程阻塞挂起操作的消耗，其它需要cup的线程又不能获取到cpu，造成cpu的浪费。所以这种情况下我们要关闭轻量级锁； 重量级锁 重量级锁是指当锁为轻量级锁时，另一个线程虽是自旋，自旋一定的次数任然未获得锁就会进入阻塞，该锁膨胀为重量级锁。重量级锁会让其它线程阻塞，降低性能。 ","link":"https://kily-007.github.io/post/笔记-java中的锁/"},{"title":"拼多多二面","content":"拼多多大数据岗二面 2019.8.19 刚从北京贝壳一轮游回来，晚上就来了拼多多二面通知，我把时间记成了8.20号，所以猝不及防借了电脑去面试，面试官是个比较年轻的男孩子，面试是加了微信，使用微信视频面得。 首先上来自我介绍，我大概讲了一下最近做的地铁轨道监测项目，然后针对我将的跟我提问，大概讲了二十分钟左右，然后提问加在线写代码 mysql数据库索引中的最左前缀原则； balaba。。。记不起来了都，以后一定面完就写面经呜呜呜。。。 给网址在线写代码：建立二叉搜索树； 有什么要问的，拼多多大数据岗分哪些方向，主要做些什么。 ","link":"https://kily-007.github.io/post/拼多多二面/"},{"title":"笔记-MarkDown教程","content":"1、标题 # 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 2、段落 &lt;br/&gt;人工换行 3、字体 *斜体* **粗体** ***粗斜体*** 4、分割线 *** --- 5、删除线 ~划掉文字~ 6、下划线 &lt;u&gt;下划线&lt;u&gt; 7、无序列表 *第一项 +第一项 -第一项 *第二项 +第二项 -第二项 *第三项 +第三项 -第三项 8、有序列表 1.第一项 2.第二项 9、区块 &gt;最外层 &gt;&gt;第一层嵌套 &gt;&gt;&gt;第二层嵌套 10、链接地址 &lt;https://www.baidu.com&gt; [1]:https://www.baidu.com 11、![属性文本](图片地址) 12、表格 | 表头 | 表头 | | ---- | ---- | | 单元格 | 单元格 | | 单元格 | 单元格 | 13、添加图片 ![]() ","link":"https://kily-007.github.io/post/MarkDown教程/"},{"title":"笔记-MySql","content":"MySql中常用俩种引擎 MyISAM (Indexed Sequential Access Method有索引的顺序访问方法) 不支持事务，但是每次查询都是原子的 支持表级锁，即每次操作是对整个表加锁； 存储表的总行数； 一个MYISAM表有三个文件：索引文件、表结构文件、数据文件； 采用非聚集索引，索引文件的数据域存储指向数据文件的指针。辅索引与主索引基本 一致，但是辅索引不用保证唯一性。 InnoDB 支持ACID的事务，支持事务的四种隔离级别； 支持行级锁及外键约束，因此可以支持写并发； 不存储总行数； 一个InnoDb引擎存储在一个文件空间（共享表空间，表大小不受操作系统控制，一个 表可能分布在多个文件里），也有可能为多个（设置为独立表空，表大小受操作系统 文件大小限制，一般为2G），受操作系统文件大小的限制； 主键索引采用聚集索引（索引的数据域存储数据文件本身），辅索引的数据域存储主键的值；因此从辅索引查找数据，需要先通过辅索引找到主键值，再访问辅索引；最好使用自增主键，防止插入数据时，为维持B+树结构，文件的大调整。 MyISAM与 InnoDB区别 count运算上的区别：因为MylSAM缓存有表meta-data(行数等)，因此在做COUNT(*)时对于一个结构很好的查询是不需要消耗多少资源的。而对于InnoDB来说，则没有这种缓存。 是否支持事物和崩溃后的安全恢复：MyISAM强调的是性能，每次查询具有原子性，其执行速度比InnoDB类型更快，但是不提供事物支持。但是InnoDB提供事物支持，外键等高级数据库功能。具有事物(commit)、回滚(rollback)、和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 是否支持外键： MyISAM不支持，而InnoDB支持。 MyISAM更适合读密集的表，而InnoDB更适合写密集的的表。 在数据库做主从分离的情况下，经常选择MyISAM作 为主库的存储引擎。 一般来说，如果需要事务支持，并且有较高的并发读取频率(MyISAM的表锁的粒度太大，所以 当该表写并发量较高时，要等待的查询就会很多了)，InnoDB是不错的选择。如果你的数据量很大（MyISAM支持压 缩特性可以减少磁盘的空间占用），而且不需要支持事务时，MyISAM是好的选择。 MySQL的基本存储结构 页的组成部分 MySQL的基本存储结构是页（记录都存在页里），页的基本结构如下图： 一个页面的存储由以下几部分组成： 名称 中文名 占用空间大小 简单描述 File Header 文件头 38字节 描述页的信息 Page Header 页头 56字节 页的状态信息 Infimum + SupreMum 最小记录和最大记录 26字节 两个虚拟的行记录（后面会说明） User Records 用户记录 不确定 实际存储的行记录内容 Free Space 空闲空间 不确定 页中尚未使用的空间 Page Directory 页目录 不确定 页中的记录相对位置 File Trailer 文件结尾 8字节 结尾信息 页中的存储 当我们在存储数据的时候，记录会存储到User Records部分 。但是在一个页新形成的时候是不存在User Records 这个部分的，每当我们在插入一条记录的时候，都会从Free Space中去申请一块大小符合该记录大小的空间并划分到User Records，当Free Space的部分空间全部被User Records部分替换掉之后，就意味着当前页使用完毕，如果还有新的记录插入，需要再去申请新的页，过程如下： 每个数据页可以组成一个双向列表； 每个数据页中的记录又可以生成一个单向列表： 每个数据页都会为存储在它里边的记录生成一个页目录，在通过主键查找某条记录的时候可以在页目录使用二分法快速定位到对应的槽，然后再遍历该槽中对应分组中的记录即可快速找到指定的记录。 以其他列(非主键)作为搜索条件：只能从最小记录开始依次遍历单链表中的每条记录 所以说，如果我们写select * from user where indexname = 'xxx’这样没有进行任何优化的sql语句，默认会这样做： 1.定位到记录所在的页:需要遍历双向链表，找到所在的页 2.从所在的页内中查找相应的记录:由于不是根据主键查询，只能遍历所在页的单链表了 很明显，在数据量很大的情况下这样查找会很慢！这样的时间复杂度为O（n）。 使用索引之后，索引做了些什么可以让我们查询加快速度呢？其实就是将无序的数据变成有序(相对)： 要找到id为8的记录简要步骤： 很明显的是：没有用索引我们是需要遍历双向链表来定位对应的页，现在通过 “目录” 就可以很快地定位到对应的页 上了！（二分查找，时间复杂度近似为O(logn)） 其实底层结构就是B+树，B+树作为树的一种实现，能够让我们很快地查找出对应的记录。 数据库四大特性 原子性（Atomicity） 原子性是指事物包含的所有操作要么全部成功，要么全部失败回滚。因此事物的操作如果成功就必须完全应用到数据库，如果操作失败则不能对数据库有任何影响。 一致性（Consistency） 一致性是指事物必须使数据库从一个一致状态变换到另一个一致性状态，也就是说一个事物执行之前和执行之后都必须处于一致性状态。 隔离性（Isolation） 多个事物并发操作时，多个并发事物之间要相互隔离。 持久性（Durability） 持久性是指一个事物一旦提交了，那么对数据库中的数据的改变就是永久性的。 数据库的四种隔离级别 读取未提交（Read Uncommitted） 在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。这是万万不可出现的。 读已提交（Read Committed） 这是大多数数据库系统的默认隔离级别（但不是MySql默认的）。它满足了隔离的简单定义：一个事物只能看见已经提交的事务所做的改变。但是该隔离级别可能会产生不可重复读的现象，即同一事务的其他实例在该实例处理期间可能会有新的commit对数据更新，所以同一select可能返回不同的结果。 采用MVCC多版本并发控制解决不可重复读的问题。 可重读（Repeatable Read） 这是MySql的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。但这会导致里另一个问题：幻读。幻读是指用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影”行。 可串行化（Serializable） 这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加共享锁，在这个级别，可能导致大量的超时现象和锁竞争。 最左前缀原则 MySQL中的索引可以以一定顺序引用多列，这种索引叫作联合索引。如User表的name和city加联合索引就是 (name,city)o而左前原则指的是，如果查询的时候查询条件精确匹配索引的左边连续一列或几列，则此列就可以 被用到。如下： select * from user where name=xx and city=xx ; ／／可以命中索引 select * from user where name=xx ; // 可以命中索引 select * from user where city=xx; // 无法命中索引 这里需要注意的是，查询的时候如果两个条件都用上了，但是顺序不同，如 city= xx and name ＝xx ，那么现在 的查询引擎会自动优化为匹配联合索引的顺序，这样是能够命中索引的. 由于左前原则，在创建联合索引时，索引字段的顺序需要考虑字段值去重之后的个数，较多的放前面。 ORDERBY子句也遵循此规则。 注意避免冗余索引：冗余索引指的是索引的功能相同，能够命中 就肯定能命中 ，那么 就是冗余索引如（name,city ）和（name ）这两 个索引就是冗余索引，能够命中后者的查询肯定是能够命中前者的 在大多数情况下，都应该尽量扩展已有的索引而 不是创建新索引。 MySQLS.7 版本后，可以通过查询 sys 库的 schemal_r dundant_indexes 表来查看冗余索引 MySql如何为表字段添加索引 1.添加PRIMARY KEY（主键索引） ALTER TABLE `table_name` ADD PRIMARY KEY ( `column` ) 2.添加UNIQUE(唯一索引) ALTER TABLE `table_name` ADD UNIQUE ( `column` ) 3.添加INDEX(普通索引) ALTER TABLE `table_name` ADD INDEX index_name ( `column` ) 4.添加FULLTEXT(全文索引) ALTER TABLE `table_name` ADD FULLTEXT ( `column`) 5.添加多列索引 ALTER TABLE `table_name` ADD INDEX index_name ( `column1`, `column2`, `column3` ) 索引 聚集索引 在聚集索引中，表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表只能包含一 个聚集索引。 如果某索引不是聚集索引，则表中行的物理顺序与键值的逻辑顺序不匹配。与非聚集索引相比，聚集索引通常提供更快的数据访问速度。 辅助索引 辅助索引与聚集索引的区别在于辅助索引的叶子节点并不包含行记录的全部数据，而是存储相应行数据的聚集索引键，即主键。当通过辅助索引来查询数据时，InnoDB存储引擎会遍历辅助索引找到主键，然后再通过主键在聚集索引中找到完整的行记录数据。 hash索引 哈希索引（hash index）基于哈希表实现，只有精确匹配索引所有列的查询才有效。对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（hash code），哈希码是一个较小的值，并且不同键值的行计算出来的哈希码也不一样。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。 hash索引的示意图： hash索引的限制： 哈希索引只包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行。 哈希索引数据并不是按照索引值顺序存储的，所以也就无法用于排序。 哈希索引也不支持部分索引列匹配查找，因为哈希索引始终是使用索引列的全部内容来计算哈希值的。 哈希索引只支持等值比较查询，包括=、IN()、&lt;&gt;（注意&lt;&gt;和&lt;=&gt;是不同的操作）。也不支持任何范围查询，例如WHERE price&gt;100。 访问哈希索引的数据非常快，除非有很多哈希冲突（不同的索引列值却有相同的哈希值）。当出现哈希冲突的时候，存储引擎必须遍历链表中所有的行指针，逐行进行比较，直到找到所有符合条件的行。 如果哈希冲突很多的话，一些索引维护操作的代价也会很高。例如，如果在某个选择性很低（哈希冲突很多）的列上建立哈希索引，那么当从表中删除一行时，存储引擎需要遍历对应哈希值的链表中的每一行，找到并删除对应行的引用，冲突越多，代价越大。 B-树 在说B+树之前我们先介绍一下B树与B+树的数据结构。 B-树定义： 一棵m阶的B-Tree有如下特性： 每个节点最多有m个孩子。 除了根节点和叶子节点外，其它每个节点至少有Ceil(m/2)个孩子。 若根节点不是叶子节点，则至少有2个孩子 所有叶子节点都在同一层，且不包含其它关键字信息 每个非终端节点包含n个关键字信息（P0,P1,…Pn, k1,…kn） 关键字的个数n满足：ceil(m/2)-1 &lt;= n &lt;= m-1 ki(i=1,…n)为关键字，且关键字升序排序。 Pi(i=1,…n)为指向子树根节点的指针。P(i-1)指向的子树的所有节点关键字均小于ki，但都大于k(i-1)。 B-Tree中的每个节点根据实际情况可以包含大量的关键字信息和分支，如下图所示为一个3阶的B-Tree： 每个节点占用一个盘块的磁盘空间，一个节点上有两个升序排序的关键字和三个指向子树根节点的指针，指针存储的是子节点所在磁盘块的地址。两个关键词划分成的三个范围域对应三个指针指向的子树的数据的范围域。以根节点为例，关键字为17和35，P1指针指向的子树的数据范围为小于17，P2指针指向的子树的数据范围为17~35，P3指针指向的子树的数据范围为大于35。 模拟查找关键字29的过程： 根据根节点找到磁盘块1，读入内存。【磁盘I/O操作第1次】 比较关键字29在区间（17,35），找到磁盘块1的指针P2。 根据P2指针找到磁盘块3，读入内存。【磁盘I/O操作第2次】 比较关键字29在区间（26,30），找到磁盘块3的指针P2。 根据P2指针找到磁盘块8，读入内存。【磁盘I/O操作第3次】 在磁盘块8中的关键字列表中找到关键字29。 分析上面过程，发现需要3次磁盘I/O操作，和3次内存查找操作。由于内存中的关键字是一个有序表结构，可以利用二分法查找提高效率。而3次磁盘I/O操作是影响整个B-Tree查找效率的决定因素。B-Tree相对于AVLTree缩减了节点个数，使每次磁盘I/O取到内存的数据都发挥了作用，从而提高了查询效率。 数据库系统的设计者巧妙利用了磁盘预读原理， 将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧： 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里， 加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 B+树 B+Tree是在B-Tree基础上的一种优化，使其更适合实现外存储索引结构，InnoDB存储引擎就是用B+Tree实现其索引结构。 从上一节中的B-Tree结构图中可以看到每个节点中不仅包含数据的key值，还有data值。而每一个页的存储空间是有限的，如果data数据较大时将会导致每个节点（即一个页）能存储的key的数量很小，当存储的数据量很大时同样会导致B-Tree的深度较大，增大查询时的磁盘I/O次数，进而影响查询效率。在B+Tree中，所有数据记录节点都是按照键值大小顺序存放在同一层的叶子节点上，而非叶子节点上只存储key值信息，这样可以大大加大每个节点存储的key值数量，降低B+Tree的高度。 B+Tree相对于B-Tree有几点不同： 非叶子节点只存储键值信息。 所有叶子节点之间都有一个链指针。 数据记录都存放在叶子节点中。 将上一节中的B-Tree优化，由于B+Tree的非叶子节点只存储键值信息，假设每个磁盘块能存储4个键值及指针信息，则变成B+Tree后其结构如下图所示： 通常在B+Tree上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点，而且所有叶子节点（即数据节点）之间是一种链式环结构。因此可以对B+Tree进行两种查找运算：一种是对于主键的范围查找和分页查找，另一种是从根节点开始，进行随机查找。 可能上面例子中只有22条数据记录，看不出B+Tree的优点，下面做一个推算： InnoDB存储引擎中页的大小为16KB，一般表的主键类型为INT（占用4个字节）或BIGINT（占用8个字节），指针类型也一般为4或8个字节，也就是说一个页（B+Tree中的一个节点）中大概存储16KB/(8B+8B)=1K个键值（因为是估值，为方便计算，这里的K取值为〖10〗3）。也就是说一个深度为3的B+Tree索引可以维护103 * 10^3 * 10^3 = 10亿 条记录。 实际情况中每个节点可能不能填充满，因此在数据库中，B+Tree的高度一般都在24层。MySQL的InnoDB存储引擎在设计时是将根节点常驻内存的，也就是说查找某一键值的行记录时最多只需要13次磁盘I/O操作。 数据库中的B+Tree索引可以分为聚集索引（clustered index）和辅助索引（secondary index）。上面的B+Tree示例图在数据库中的实现即为聚集索引，聚集索引的B+Tree中的叶子节点存放的是整张表的行记录数据。辅助索引与聚集索引的区别在于辅助索引的叶子节点并不包含行记录的全部数据，而是存储相应行数据的聚集索引键，即主键。当通过辅助索引来查询数据时，InnoDB存储引擎会遍历辅助索引找到主键，然后再通过主键在聚集索引中找到完整的行记录数据 局部性原理与磁盘预读 由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速 度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目 的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的**局部性原理：**当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间 所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统 往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系 统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 常见大表的优化手段 当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下： 限定数据的范围 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时 候，我们可以控制在一个月的范围内。 读写分离 经典的数据库拆分方案，主库负责写，从库负责读。 垂直分区 根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信 息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。简单来说垂直拆分是指数据表列的拆分， 把一张列比较多的表拆分为多张表。 垂直拆分的优点： 可以使得行数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简 化表的结构，易于维护。垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过 在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂； 水平分区 保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达 到了分布式的目的。 水平拆分可以支撑非常大的数据量。 水平拆分是指数据表行的拆分，表的行数超过200万 行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多 个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。 水平拆分可以支持非常大的数据量。需要注意的一点是:分表仅仅是解决了单一表数据过大的问题，但由于表的 数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 水平拆分最好分库 。水平拆分能 够 支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨界点Join性能较差，逻辑复杂。 《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂 度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择 客户端分片架构，这样可以减少一次和中间件的网络I/O。 数据库分片的两种常见方案 客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 ShardingJDBC 、阿里的TDDL是两种比较常用的实现。 中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。 Sql注入和防注入 Sql注入 简介 SQL注入是比较常见的网络攻击方式之一，它不是利用操作系统的BUG来实现攻击，而是针对程序员编程时的疏忽，通过SQL语句，实现无帐号登录，甚至篡改数据库。 SQL注入攻击的总体思路 1.寻找到SQL注入的位置 2.判断服务器类型和后台数据库类型 3.针对不通的服务器和数据库特点进行SQL注入攻击 SQL注入攻击实例 比如在一个登录界面，要求输入用户名和密码： 可以这样输入实现免帐号登录： 用户名： ‘or 1 = 1 – 密 码： 点登陆,如若没有做特殊处理,那么这个非法用户就很得意的登陆进去了.(当然现在的有些语言的数据库API已经处理了这些问题)这是为什么呢? 下面我们分析一下：从理论上说，后台认证程序中会有如下的SQL语句： String sql = &quot;select * from user_table where username=' &quot;+userName+&quot; ' and password=' &quot;+password+&quot; '&quot;; //当输入了上面的用户名和密码，上面的SQL语句变成： SELECT * FROM user_table WHERE username='’or 1 = 1 -- and password='’ 分析SQL语句： 条件后面username=”or 1=1 用户名等于 ” 或1=1 那么这个条件一定会成功；然后后面加两个-，这意味着注释，它将后面的语句注释，让他们不起作用，这样语句永远都能正确执行，用户轻易骗过系统，获取合法身份。这还是比较温柔的，如果是执行SELECT * FROM user_table WHEREusername='' ;DROP DATABASE (DB Name) --' and password='' ….其后果可想而知… 防注入方法 预编译语句集（PreparedStatement） 采用预编译语句集，它内置了处理SQL注入的能力，只要使用它的setXXX方法传值即可。 使用好处： (1).代码的可读性和可维护性. (2).PreparedStatement尽最大可能提高性能. (3).最重要的一点是极大地提高了安全性.原理： PreparedStatement preparedStatement =connection.prepareStatement(preSql); preparedStatement.setString(1, username); preparedStatement.setString(2, pwd); 原理： sql注入只对sql语句的准备(编译)过程有破坏作用 而PreparedStatement已经准备好了,执行阶段只是把输入串作为数据处理, 而不再对sql语句进行解析,准备,因此也就避免了sql注入问题. 使用正则表达式过滤传入的参数 下面是具体的正则表达式： 检测SQL meta-characters的正则表达式 ： /(%27)|(\\’)|(--)|(%23)|(#)/ix 修正检测SQL meta-characters的正则表达式 ：/((%3D)|(=))[^\\n]*((%27)|(\\’)|(--)|(%3B)|(😃)/i 典型的SQL 注入攻击的正则表达式 ：/\\w*((%27)|(\\’))((%6F)|o|(%4F))((%72)|r|(%52))/ix 检测SQL注入，UNION查询关键字的正则表达式 ：/((%27)|(\\’))union/ix(%27)|(\\’) 检测MS SQL Server SQL注入攻击的正则表达式： /exec(\\s|+)+(s|x)p\\w+/ix 字符串过滤 过滤掉一些敏感特殊字符关键字 String inj_str = &quot;'|and|exec|insert|select|delete|update| count|*|%|chr|mid|master|truncate|char|declare|;|or|-|+|,&quot;; String inj_stra[] = split(inj_str,&quot;|&quot;); ","link":"https://kily-007.github.io/post/笔记-MySql/"},{"title":"格力一面面经","content":"自我介绍 首先上来是面试官自我介绍，然后让我自我介绍，主要说了所在实验室和平时所做的 项目，在介绍的过程中涉及到了平时所做的项目，我这边主要是hadoop+kafka+hbase的大数据相关的内容的一个项目和很久以前做的MVC架构的酒店管理系统。 由项目展开详细追问 主要在我叙说项目过程中，会时不时的问到相关的专业知识内容。我这里主要被问到有udp的数据传输原理，以及为什么使用这样的一个协议。这里回答为UDP的即时通讯原理，和面向无连接的要求速度快的这样一个应用场景。 kafka数据传输原理 问：包含数据如何打包，key值设计原理，会问为什么这样设计，包含生产者和消费者具体如何对接。 答：kafka以键值对的形式进行数据传输，因为本身kafka是一个消息队列机制，因为项目中对分区没有要求，所以采用单分区的形式，key值中设计为数据的属性信息，key=仪表号-通道号-时间戳，key值采用string类型进行传输，value为一个float类型数组，在传输前将float数组转为byte字节，采用字节流的形式进行传输。在消费者端对key值进行解析，把value值解析回float数组，其实value值就是一个典型序列化的过程，消费者端通过解析key值将数据value存储到相应的地方。 hbase非关系型数据库 主要涉及到hbase的查询优化以及如何避免scan全表扫描策略，我这里回答使用filter过滤器，在面试官提醒下想起来有起始行startrow和结束行endrow对扫面范围进行限定这样一种策略。 过滤器包括： 行键过滤器：RowFilter 列簇过滤器 FamilyFilter 列过滤器 QualifierFilter 值过滤器 ValueFilter 时间戳过滤器 TimestampsFilter 前缀过滤器 PrefixFilter----针对行键 //设置开始行（前缀）和结束行（前缀） Scan scan = new Scan();//全表扫描器 scan.setStartRow(&quot;startRowKey&quot;.getBytes()); scan.setStopRow(&quot;stopRowKey&quot;.getBytes()); hadoop中涉及到的守护进程 master: NameNode,SecondaryNameNode,ResourceManager slave1:DataNode,NodeManager slave2:DataNode,NodeManager 说说对数据库中事务的理解 这个还没复习到，没答上来，下面贴答案： 事务：就是被绑定在一起作为一个逻辑工作单元的 SQL 语句分组，如果任何一个语句操作失败那么整个操作就被失败，以后操作就会回滚到操作前状态，或者是上有个节点。为了确保要么执行，要么不执行，就可以使用事务。要将有组语句作为事务考虑，就需要通过 ACID 测试，即原子性，一致性，隔离性和持久性。 锁：在所以的 DBMS 中，锁是实现事务的关键，锁可以保证事务的完整性和并发性。与现实生活中锁一样，它可以使某些数据的拥有者，在某段时间内不能使用某些数据或数据结构。当然锁还分级别的。 事务必须服从ISO/IEC所制定的ACIO原则，ACIO是原子性（atomicity）、一致性（consistency）、隔离性（isolation）、持久性（durability）的缩写。 事务的原子性：表示事务执行过程中的任何失败都将导致事务所做的任何修改失效。 事务的一致性：表示当事务执行失败时，所有被该事务影响的数据都应该恢复到事务执行前的状态。 事务的隔离性：表示在事务执行过程中对数据的修改，在事务提交之前对其他事务不可见。 事务的持久性：表示已提交的数据在事务执行失败时，数据的状态都应该是正确的。 事务并发操作带来的问题 2）脏读： 一个事务读取了另一个事务未提交的数据。 3）不可重复读：一个事务两次读取同一个数据，两次读取的数据不一致。 4）幻象读： 一个事务两次读取一个范围的记录，两次读取的记录数不一致。 数据库连接jdbc中，如何实现事务 当一个连接对象被创建时默认情况下是自动提交事务，每次执行一个SQL语句时，如果执行成功，就会向数据库自动提交，而不能回滚，为了让多个SQL语句作为一个事务执行，可使用一下步骤： ——调用Connection对象的setAutoCommit(false)，以取消自动提交事务。 ——在所有的SQL语句都成功执行后，调用commit方法提交事务。 ——在出现异常时，调用rollback方法，事务回滚。 ——若此时Conneciton没有关闭，则需要恢复其自动提交状态。 注意：若使用的数据库引擎不是innodb，则事务无法回滚。而mysql默认的数据库引擎是MyISAM，所以第一次使用事务则需要更改数据表的引擎。 ","link":"https://kily-007.github.io/post/格力一面面经/"},{"title":"笔记-java基础","content":"1 java基础知识 1.2 重载和重写的区别 重载：相同的函数名，参数类型不同 重写：子类重写父类方法，private修饰的方法不能重写 1.3 string与stringbuild与stringBuffer区别 String 中由final修饰的char数组实现，故不可变； stringBuild比stringBuffer性能好一丢丢； StringBuffer为线程安全的，其对方法加了同步锁或者对调用方法加了同步锁 1.4 ==与equals == 基本类型比较的是值，对象（引用数据类型）比较的是地址； equals() : 未重写的equals方法和==等价，重写之后的equals比较的是对象的值是否相 1.41 为什么重写equals方法，一定要重写HashCode方法 如果你重载了equals，比如说是基于对象的内容实现的，而保留hashCode的实现不变，那么很可能某两个对象明明是“相等”，而hashCode却不一样。 这样，当你用其中的一个作为键保存到hashMap、hasoTable或hashSet中，再以“相等的”找另一个作为键值去查找他们的时候，则根本找不到。 使用HashMap，如果key是自定义的类，就必须重写hashcode()和equals()。 而对于每一个对象，通过其hashCode()方法可为其生成一个整形值（散列码），该整型值被处理后，将会作为数组下标，存放该对象所对应的Entry（存放该对象及其对应值）。 equals()方法则是在HashMap中插入值或查询时会使用到。当HashMap中插入值或查询值对应的散列码与数组中的散列码相等时，则会通过equals方法比较key值是否相等，所以想以自建对象作为HashMap的key，必须重写该对象继承object的hashCode和equals方法。 2.本来不就有hashcode()和equals()了么？干嘛要重写，直接用原来的不行么？ HashMap中，如果要比较key是否相等，要同时使用这两个函数！因为自定义的类的hashcode()方法继承于Object类，其hashcode码为默认的内存地址，这样即便有相同含义的两个对象，比较也是不相等的，例如，生成了两个“羊”对象，正常理解这两个对象应该是相等的，但如果你不重写 hashcode（）方法的话，比较是不相等的！ HashMap中的比较key是这样的，先求出key的hashcode(),比较其值是否相等，若相等再比较equals(),若相等则认为他们是相等的。若equals()不相等则认为他们不相等。如果只重写hashcode()不重写equals()方法，当比较equals()时只是看他们是否为同一对象（即进行内存地址的比较）,所以必定要两个方法一起重写。HashMap用来判断key是否相等的方法，其实是调用了HashSet判断加入元素是否相等。 1.5 final关键字的使用：变量，方法，类 变量：修饰基本数据类型，则初始化之后不可更改；修饰引用类型变量，则初始化之后不能让其指向另一个对象。 方法：锁定方法，以防任何继承类修改 类：表明该类不能被继承，且类中所有方法均被隐式指定为final方法 1.6 object类常见方法 public final native Class&lt;?&gt; getClass() //用于返回当前运行时对象的calss对象 public native int hashCode() //返回对象hash码 public boolean equals(Object obj) //比较俩个对象的内存地址是否相等 protected native Object clone() throws CloneNotSupportedExcception // public String toString() //返回类名@实例的哈希码的16进制的字符串 public final native void notify() //唤醒一个在此对象监视器上等待的县城，如果有多个线程在等待只会唤醒一个 public final native void notifyAll() public final native void wait(long timeout) throws InterrupteException //暂停线程的执行。注意：sleep方法没有释放锁，wait方法释放了锁 public final void wait(long timeout,int nanos) throws InterrupteException //nanos表示额外时间 public final void wait() throws InterrupteException //一直等待，没有超时概念 protected void finalize() throws Throwable { } //实例被垃圾回收器回收的时候触发的操作 1.7 java中常见异常 所有异常的共同祖先Throwable类，其俩个重要子类： Eception（异常）和Error（错误） Error：程序无法处理的错误，大多数与编码者执行操作无关 Exception：程序本身可以 处理的异常 异常处理：无论异常是否捕获或处理，finally块里的语句都会执行。挡在try或catch块中遇到return语句时，finally语句块将在方法返回之前被执行 注意：以下四种特殊情况finally不会被执行： 在finally块语句中发生了异常； 在前面的代码中用了System,exit()退出程序； 程序坐在线程死亡； 关闭CPU。 1.8 java中的俩种输入常用方法 Scanner input = new Scanner(System.in); String s = input.nextLine(); input.close(); BufferedReader input = new BufferedReader(new InputStreamReader(System.in)); String s = input.readLine(); 1.9 接口和抽象类的区别 一个类可以实现多个接口；接口中的方法只有声明，不能实现；要实现接口中的所有方法；接口中的实例变量默认是final类型的 接口：接口只有方法体没有具体的实现，用interface修饰 特点： 接口中没有自己的属性，只能有方法体 类在实现接口时，可以实现多个接口 //接口 UserRightsService.java public interface UserRightsService { // 获取数据总数 public int findUserRightsCount() throws Exception; // 根据开始位置，每页记录数查找用户列表 public List&lt;Userrights&gt; selectUserRightsList(int startPos, int pageSize) throws Exception; // 根据用户权限的id查找一条数据 public Userrights findUserRightsById(int id) throws Exception; // 根据id更新一条数据 public void updateUserRightsById(int id, String username) throws Exception; // 根据传过来的参数查询对应参数的数据 public String checkDataByParam(String param, String userloginid) throws Exception; } //实现接口 UserRightsServiceImpl.java public class UserRightsServiceImpl implements UserRightsService { @Autowired private UserrightsVoMapper userrightsVoMapper; // 获取数据总数 public int findUserRightsCount() throws Exception { return userrightsVoMapper.findUserRightsCount(); } // 根据开始位置，每页记录数查找用户列表 public List&lt;Userrights&gt; selectUserRightsList(int startPos, int pageSize) throws Exception { return userrightsVoMapper.selectUserRightsList(startPos, pageSize); } // 根据用户权限的id查找一条数据 public Userrights findUserRightsById(int id) throws Exception { return userrightsVoMapper.findUserRightsById(id); } // 根据id更新一条数据 public void updateUserRightsById(int id,String username) throws Exception { userrightsVoMapper.updateUserRightsById(id,username); } // 根据传过来的参数查询对应参数的数据 public String checkDataByParam(String param, String userloginid) throws Exception { return userrightsVoMapper.checkDataByParam(param, userloginid); } 一个类只能实现一个抽象类，抽象类中可以有非抽象方法； 抽象类：指的是用abstract关键字修饰或者类中有抽象方法，那么这个类就是抽象类 特点： 抽象类用关键字 abstract修饰 抽象类的抽象方法没有方法体，在子类继承父类中有抽象方法时，必须实现抽像方法 抽象类不能被实例化 //抽象类 Employee.java public abstract class Employee{ private String name; private String address; private int number; /* 声明抽象方法会造成以下两个结果： 1.如果一个类包含抽象方法，那么该类必须是抽象类。 2.任何子类必须重写父类的抽象方法，或者声明自身为抽象类。 */ public abstract double computePay(); public string getName(){ return name; } } //继承抽象类 Salary.java public class Salary extends Employee{ private double salary; // Annual salary //任何子类必须重写父类的抽象方法 public double computePay(){ System.out.println(&quot;Computing salary pay for &quot; + getName()); return salary/52; } } 1.10 (B)IO、NIO、AIO (B)IO：同步阻塞I/O，同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。并发局限于应用中，在jdk1.4以前是唯一的io NIO：同步非阻塞I/O，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有IO请求时才启动一个线程进行处理。NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，jdk1,4开始支持。 AIO：异步非阻塞I/O，服务器实现模式为一个有效请求一个线程，客户端的IO请求都是由操作系统先完成了再通知服务器用其启动线程进行处理。AIO方式适用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，jdk1.7开始支持 同步：如果有多个任务或者事件要发生，这些任务或者事件必须逐个地进行，一个事件或者任务的执行会导致整个流程的暂时等待，这些事件没有办法并发地执行； 异步：如果有多个任务或者事件发生，这些事件可以并发地执行，一个事件或者任务的执行不会导致整个流程的暂时等待。 阻塞 / 非阻塞描述的是函数，指访问某个函数时是否会阻塞线程(block，线程进入阻塞状态)。 同步 /异步描述的是执行IO操作的主体是谁，同步是由用户进程自己去执行最终的IO操作。异步是用户进程自己不关系实际IO操作的过程，只需要由内核在IO完成后通知它既可，由内核进程来执行最终的IO操作。 https://blog.csdn.net/z15732621582/article/details/78939122?tdsourcetag=s_pctim_aiomsg 1 ) 异步非阻塞例子:(网上看到的比较短小精悍的好例子,直接拿过来了)老张爱喝茶，废话不说，煮开水。 出场人物：老张，水壶两把（普通水壶，简称水壶；会响的水壶，简称响水壶）。 1 老张把水壶放到火上，原地不动等水开。（同步阻塞） ----------&gt;老张觉得自己有点傻 2 老张把水壶放到火上，去客厅看毛骗，时不时去看看水开没有。（同步非阻塞） ----------&gt;老张觉得自己有点傻 于是变高端了，买了把会响笛的那种水壶。水开之后，能大声发出嘀~~~~的响声。 3 老张把响水壶放到火上，立等水开。（异步阻塞） ---------&gt;老张觉得自己有点傻 4 老张把响水壶放到火上，去客厅看毛骗，水壶响之前不再去看它，响了再去拿壶。（异步非阻塞） ----------&gt;嗯,老张觉得自己棒棒哒 2 java容器 2.1 Arraylist与LinkList的异同 ArrayList初始化长度为10，每次扩容为原来的1.5倍。 均为线程不安全的 Arraylist底层采用Object数组实现；LinkedList采用双向链表数据结构实现 Arraylist插入默认追加到末尾，时间复杂度为O(1)，若插入到指定位置，就需要将指定位置后的所有元素后移一位；LinkList插入时间复杂恒为O(1) 快速随机访问 空间占用 //RandomAccess接口中什么都没有定义，可以理解为一个是否具有随机访问功能的标识 public interface RandomAccess{ } 迭代器模式：就是提供一种方法对一个容器对象中的各个元素进行访问，而又不暴露该对象容器的内部细节。 2.2 Vector类 Vector类和ArrayList类似，都是由动态数组实现； 是线程安全的，它的方法之间是线程同步的。 Vector扩容时增长为原来的2倍，ArrayList扩容为原来的1.5倍 2.3 hashmap JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashCode 经 过扰动函数处理过后得到 hash 值，然后通过 (n - 1) &amp; hash 判断当前元素存放的位置（这里的 n 指的是数组的 长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的 话，直接覆盖，不相同就通过拉链法解决冲突。 所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。 HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap. 2.4 java集合总结 Collection List ArrayList：Objct数组实现 Vector：Object实现，线程安全 LinkList：双向链表 Set HashSet：基于hashmap实现， 无序、唯一 LinkedHashSet：继承 HashSet，内部为LinkedHashMap实现 TreeSet：红黑树，有序、唯一 Map Hashmap：数组+链表+红黑树 LinkedHashMap：继承HashMap，双向链表+数组，插入有序或访问有序 HashTable：synchronized同步的HashMap TreeMap：红黑树 3 java 多线程关键字 3.1 synchronized关键字 修饰实例方法，作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁。 修饰静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁。也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员(所以如果一个线程A调用一个实 例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允 许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 ) 修饰代码块，指定加锁对象，对给定的对象加锁，进入同步代码库前要获得给定对象的锁。（synchronized加到static静态方法和synchronized(class)代码块都是给Class类上锁） 双重校验锁实现对象单例（线程安全） public class Singleton{ private volatile static Singleton uniqueInstance; private Singleton(){} public static Singleton getUniqueInstance(){ if(uniqueInstance==null){ synchronized(Singeton.class){ if(uniqueInstance==null){ uniqueInstance=new Singleton(); } } } return uniqueInstance; } } 另外，需要注意 uniqueInstance 采用 volatile 关键字修饰也是很有必要。 uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分 为三步执行：1.为 uniqueInstance 分配内存空间 2. 初始化 uniqueInstance 3. 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-&gt;3-&gt;2。指令重排在单线程环境下不会出先问题，但是在 多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被 初始化。 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。 3.2 ReenTrantLock ReenTrantLock 是可重入的，其加锁和解锁是手动操作，比较灵活。而synchronized的加锁解锁过程是隐式的。 ReentrantLock lock = new ReentrantLock(true);//可通过传参指定该锁是公平锁还是非公平锁。 //公平锁是指当锁可用时,在锁上等待时间最长的线程将获得锁的使用权 3.3 volatile volatile关键字是线程同步的轻量级实现，所以性能肯定比synchronized好。但volatile只能修饰变量，而synchronized关键字能修饰方法和代码块。（synchronized关键字在JavaSE1.6之后进 行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁以及其它各种优化之后执行 效率有了显著提升，实际开发中使用 synchronized 关键字的场景还是更多一些。 ） volatile关键字能保证数据的可见性，单不能保证数据的原子性，synchronized两者都能保证。 volatile主要用于解决变量在多个线程之间的可见性，而synchronized解决的是多个线程之间访问资源的同步性。 3.4 使用线程池的优点 降低资源消耗。 通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。 当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。 线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性， 使用线程池可以进行统一的分配，调优和监控。 3.5 实现Runnable接口与Callable接口的区别 Runnable接口不会返回结果；Callable接口可以返回结果。 3.6 execute()方法和submit()方法有什么区别 execute() 方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功 submit()方法用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过该对象可以判断任务是否成功执行。 3.7 如何创建线程池 《阿里巴巴Java开发手册》中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。 方式一：通过构造方法创建： ThreadPoolExecutor( int corePoolSize,//指定线程池中线程的数量 int maximumPoolSize,//最大线程数量 long keepAliveTime,//空闲线程数量超过corePoolSize时，多余线程多久被销毁 TimeUnit unit,//KeepAliveTime的单位 BlockingQueue&lt;Runnable&gt; workQueue,//任务队列 ThreadFactory threadFactory,//线程工厂 RejectedExecutionHandler handler ) //拒绝策略 方式二：通过Executors创建 FixedThreadPool：返回固定线程数量的线程池。 SingleThreadExecutor：返回一个只有一个线程的线程池。 CachedThreadPool：该方法返回一个可根据实际情况调整线程数量的线程池。线程池的线程数量不确定，但 若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新 的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。 3.8 Atomic原子类 并发包java.util.concurrent的原子类都放在java.util.concurrent.atomic下，JUC包中的原子类包含四类： 基本类型：使用院子的方式更新基本类型 AtomicInteger：整形原子类 AtomicLong：长整型原子类 AtomicBoolean：布尔型原子类 数组类型：使用院子方式是更新数数组里的某个元素 AtomicIntegerArray：整形数组原子类 AtomicLongArray：长整型数组原子类 AtomicReferenceArray：引用类型数组原子类 引用类型： AtomicReference：引用类型院子类 AtomicStampedRerence：原子更新引用类型例的字段原子类 AtomICMarkableReference：原子更新带有标记的引用类型，例：AtomicReferenceArray 对象的属性修改类型： AtomicIntegerFieldUpdater:原子更新整形字段的更新器 AtomicLongFieldUpdater：原子更新长整形字段的更新器 AtomicStampedReference ：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原 子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 AtomicInter类常用方法 常用方法： public final int get() //获取当前的值 public final int getAndSet(int newValue)//获取当前的值，并设置新的 public final int getAndIncrement()//获取当前的值，并自增 public final int getAndDecrement() //获取当前的值，并自减 public final int getAndAdd(int delta) //获取当前的值，并加上预期的 boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置 为输入值（update） public final void lazySet(int newValue)//终设置为newValue,使用 lazySet 设置之后可能导致其他线 程在之后的一小段时间内还是可以读到旧的值。 使用示例 ： class AtomicIntegerTest { private AtomicInteger count = new AtomicInteger(); //使用AtomicInteger之后，不需要对该方法加锁，也可以实现线程安全。 public void increment() { count.incrementAndGet(); } public int getCount() { return count.get(); } } 3.9 AQS原理 简介（AbstractQueuedSynchronizer） AQS是一个用来构建锁和同步器的框架，使用AQS能简单且高效的构建出应用广泛的大量的同步器，例如ReentrantLock，Semaphore，其他的注入ReentrantReadWriteLock，SynchronousQueue，FutureTask等皆是基于AQS的，利用AQS我们能轻松的构建出符合需求的同步器。 原 理 AQS的核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 CLH(Craig,Landin,and Hagersten）队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅 存在结点这几件的关联关系）。AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个结点（node）来实现锁的分配。） AQS使用一个int成员变量来表示同步状态，通过内置的FIFO队列来完成获取资源线程的排队工作。AQS使用CAS对该同步状态进行原子操作实现对其值得修改。 //volatile修饰的队列头节点 private transient volatile Node head; //volatile修饰的队列尾节点 private transient volatile Node tail; //volatile修饰的同步状态 //state = 0 表示同步状态可用（如果用于锁，表示锁可用） //state = 1 表示同步状态已被占用（锁被占用） private volatile int state; 根据源码可以显然看到，AQS 内部实现了 Node 和 ConditionObject 两个内部类，并且 Node 是使用 static final修饰的静态内部类，可以看到其结构如下： Node 的源码如下： static final class Node { //共享模式 static final Node SHARED = new Node(); //独占模式 static final Node EXCLUSIVE = null; //因为超时或者中断，节点会被设置为取消状态，被取消的节点时不会参与到竞争中的，他会一直保持取消状态不会转变为其他状态； static final int CANCELLED = 1; //后继节点的线程处于等待状态，而当前节点的线程如果释放了同步状态或者被取消，将会通知后继节点，使后继节点的线程得以运行 static final int SIGNAL = -1; //节点在等待队列中，节点线程等待在Condition上，当其他线程对Condition调用了signal()后，改节点将会从等待队列中转移到同步队列中，加入到同步状态的获取中 static final int CONDITION = -2; //表示下一次共享式同步状态获取将会无条件地传播下去 static final int PROPAGATE = -3; //等待状态 volatile int waitStatus; //前驱节点 volatile Node prev; //后继节点 volatile Node next; //当前节点的线程 volatile Thread thread; } 组件 Semaphore（信号量）-允许多个线程同时访问：synchronized和ReentrantLock都是只允许一个线程访问某个资源，Semaphore可以指定多个线程同时访问某个资源。 CountDownLatch（倒计时器）：是一个同步工具类，用来协调多个线程之间的同步。通常用来控制线程等待，它可以让某一个线程等待直到倒计时结束，再开始执行。 CyclicBarrier（循环栅栏）：CyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待， 但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫 同步点）时被阻塞，直到后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。 CyclicBarrier默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用 await方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞 实现 了解了上面的核心结构，我们就可以继续来研究一下 AQS 内部具体是怎么实现的。 同步器的设计是基于模板方法设计模式的，也就是说，使用者需要继承同步器并重写指定方法，然后将同步器组合在自定义同步组件的实现中，并调用同步器提供的模板方法，而这些模板方法将会调用使用者重写的方法。可能说的有点抽象，后面在探讨 ReentrantLock 等基于 AQS 实现的类中会逐渐明白这个原理。 重写同步器指定的方法时，需要使用同步器提供的如下3个方法来访问或者修改同步状态： getState()：获取当前同步状态。 setState(int newState)：设置当前同步状态。 compareAndSetState(int expect,int update)：使用 CAS 设置当前状态，该方法能够保证状态设置的原子性。 其源码实现： //获取同步状态 protected final int getState() { return state; } //设置同步状态 protected final void setState(int newState) { state = newState; } //以原子方式设置同步状态 protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 由于 AQS 里面的方法很多，但是绝大多数方法都是 private 和 final 的，即不能被继承和实现。在第一个 AQS 图中红框圈出来的表示的是继承 AQS 后能够重写的方法，其方法名称和方法描述如下： 方法名称 描述 protected boolean tryAcquire(int args) 独占式获取同步状态，实现该方法需要查询当前状态并判断同步状态是否符合预期，然后再进行CAS设置同步状态。 protected boolean tryRelease(int arg) 独占式释放同步状态 ，等待获取同步状态的线程将有机会获取同步状态 protected int tryAcquireShared 共享式获取同步状态，返回大于等于0的值表示获取成功，否则获取失败 protected boolean tryReleaseShared(int arg) 共享式释放同步状态 protected boolean isHeldExclusively() 当前同步器是否在独占模式下被线程占用，一般该方法表示是否被当前线程独占 源码中上面方法如下： //尝试获取独占模式 protected boolean tryAcquire(int arg) { throw new UnsupportedOperationException(); } //尝试释放独占模式 protected boolean tryRelease(int arg) { throw new UnsupportedOperationException(); } //共享式获取同步状态 //返回负数表示失败;0表示成功，但没有剩余可用资源;正数表示成功，且有剩余资源。 protected int tryAcquireShared(int arg) { throw new UnsupportedOperationException(); } //共享式释放同步状态;如果释放后允许唤醒后续等待结点返回true，否则返回false。 protected boolean tryReleaseShared(int arg) { throw new UnsupportedOperationException(); } //当前同步器是否在独占模式下被线程占用，一般该方法表示是否被当前线程所独占;只有用到condition才需要去实现它。 protected boolean isHeldExclusively() { throw new UnsupportedOperationException(); } 根据上面分析可知，AQS 在内部定义了两种资源共享方式： Exclusive（独占，只有一个线程能执行，如ReentrantLock） Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch） 下面我们对这两种方式一一探讨。 独占模式 独占模式就相当于使用了排他锁，每次只有一个线程能够执行，其他线程需要在 CLH 队列中排队。下面分析一下 AQS 源码中的独占模式方法。 acquire()方法 该方法以独占模式获取共享资源。如果获取到资源，线程直接返回，否则进入等待队列，直到获取到资源为止，且整个过程忽略中断的影响。ReentrantLock的lock方法就是调用的该方法来获取锁。 方法的执行流程如下： 调用自定义同步器的tryAcquire()尝试直接去获取资源，如果成功则直接返回。 没成功，则addWaiter()将该线程加入等待队列的尾部，并标记为独占模式。 acquireQueued()使线程在等待队列中休息，有机会时（轮到自己，会被unpark()）会去尝试获取资源。获取到资源后才返回。如果在整个等待过程中被中断过，则返回true，否则返回false。 如果线程在等待过程中被中断过，它是不响应的。只是获取资源后才再进行自我中断selfInterrupt()。 /** * 独占模式获取同步状态，如果当前线程获取同步状态成功，则直接返回，否则 * 将会进入同步队列等待，该方法会调用实现类重写的tryAcquire(int arg)方法 */ public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } tryAcquire()方法 源码 doc 翻译：尝试以独占模式获取。 如果对象的状态允许以独占模式获取它，则此方法应查询，如果是，则获取它。 执行 acquire 的线程始终调用此方法。 如果此方法报告失败，则获取方法可以对线程进行排队（如果它尚未排队），直到它通过某个其他线程的释放来发出信号。 这可用于Lock类中实现tryLock()方法。 addWaiter()方法 源码 doc 翻译：为当前线程和给定模式创建节点并进行排队。 /** * 将当前线程加入到等待队列的队尾，并返回当前线程所在的结点 */ private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); // 首先尝试在链表的后面快速添加节点 Node pred = tail; if (pred != null) { node.prev = pred; // 将该节点添加到队列尾部 if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } // 如果首节点为空或者CAS添加失败，则进入enq方法通过自旋方式入队列，确保一定成功，这是一个保底机制 enq(node); return node; } enq()方法 源码 doc 翻译：将节点插入队列，必要时进行初始化 。 通过自旋锁的方式来保证节点可以正确添加，只有成功添加后，当前线程才会从该方法返回，否则会一直执行下去 。 /** * 将node加入队尾 */ private Node enq(final Node node) { // 自旋 for (;;) { Node t = tail; // 当前没有节点，构造一个new Node()，将head和tail指向它 if (t == null) { if (compareAndSetHead(new Node())) tail = head; } else { // 当前有节点，将传入的Node放在链表的最后 node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } acquireQueued()方法 源码doc翻译：对于已经在队列中的线程，以独占不间断模式获取。 由条件等待方法使用以及获取。 通过tryAcquire()和addWaiter()，该线程获取资源失败，已经被放入等待队列尾部了。下一步需要处理的是：进入等待状态休息，直到其他线程彻底释放资源后唤醒自己，自己再拿到资源，然后就可以去干自己想干的事了。其实就是个排队拿号，在等待队列中排队拿号，直到拿到号后再返回 。 final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; // 标记等待过程中是否被中断过 for (;;) { final Node p = node.predecessor(); // node的前一个节点 // 如果前一个节点是head，说明当前node节点是第二个节点，接着尝试去获取资源 // 可能是head释放完资源唤醒自己的，当然也可能被interrupt了 if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return interrupted; // 返回等待过程中是否被中断过 } // 如果自己可以休息了，就进入waiting状态，直到被unpark() if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; // 如果等待过程中被中断过，哪怕只有那么一次，就将interrupted标记为true } } finally { if (failed) cancelAcquire(node); } } shouldParkAfterFailedAcquire()方法 源码 doc 翻译：检查并更新无法获取的节点的状态。 如果线程应该阻塞，则返回 true。 这是所有获取循环中的主要信号控制。 需要pred == node.prev。 此方法主要用于检查状态，看看自己是否真的可以去休息了 如果 pred 的waitStatus是SIGNAL，直接返回true 如果 pred 的waitStatus&gt;0，也就是CANCELLED，向前一直找到&lt;= 0的节点，让节点的next指向node 如果 pred 的waitStatus&lt;=0，改成SIGNAL private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { int ws = pred.waitStatus; if (ws == Node.SIGNAL) // 如果已经告诉前驱拿完号后通知自己一下，那就可以一边玩蛋去了 return true; if (ws &gt; 0) { /* * 如果前节点放弃了，那就一直往前找，直到找到最近一个正常等待的状态，并排在它的后边。 * 注意：那些放弃的结点，由于被自己“加塞”到它们前边，它们相当于形成一个无引用链，稍后就会被GC回收 */ do { node.prev = pred = pred.prev; } while (pred.waitStatus &gt; 0); pred.next = node; } else { // 如果前节点正常，那就把前节点的状态设置成SIGNAL，告诉它拿完号后通知下。 compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false; } parkAndCheckInterrupt()方法 /** * 让线程去休息，真正进入等待状态 */ private final boolean parkAndCheckInterrupt() { LockSupport.park(this); // 调用park()使线程进入waiting状态 return Thread.interrupted(); // 如果被唤醒，查看是否被中断(该方法会重置标识位) } acquireQueued总共做了3件事： 结点进入队尾后，检查状态。 调用 park() 进入waiting 状态，等待 unpark() 或 interrupt() 唤醒自己。 被唤醒后，看自己是不是有资格能拿到号。如果拿到，head 指向当前结点，并返回从入队到拿到号的整个过程中是否被中断过；如果没拿到，继续流程 1。 流程图如下： release()方法 此方法是独占模式下线程释放资源的顶层入口。它会释放指定量的资源，如果彻底释放了（即 state=0）,它会唤醒等待队列里的其他线程来获取资源 。 /** * 释放资源 */ public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); // 唤醒等待队列里的下一个线程 return true; } return false; } tryRelease()方法 跟tryAcquire()一样，这个方法是需要独占模式的自定义同步器去实现的。正常来说，tryRelease()都会成功的，因为这是独占模式，该线程来释放资源，那么它肯定已经拿到独占资源了，直接减掉相应量的资源即可（state-=arg），也不需要考虑线程安全的问题。但要注意它的返回值，上面已经提到了，release()是根据tryRelease()的返回值来判断该线程是否已经完成释放掉资源了！所以自义定同步器在实现时，如果已经彻底释放资源（state=0），要返回 true，否则返回 false。 unparkSuccessor()方法 private void unparkSuccessor(Node node) { // 这里，node一般为当前线程所在的结点。 int ws = node.waitStatus; if (ws &lt; 0) // 置零当前线程所在的结点状态，允许失败。 compareAndSetWaitStatus(node, ws, 0); // 找到下一个需要唤醒的结点s Node s = node.next; if (s == null || s.waitStatus &gt; 0) { s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; } if (s != null) LockSupport.unpark(s.thread); // 唤醒 } 总结一下 在 AQS 中维护着一个 FIFO 的同步队列，当线程获取同步状态失败后，则会加入到这个 CLH 同步队列的对尾并一直保持着自旋。在 CLH 同步队列中的线程在自旋时会判断其前驱节点是否为首节点，如果为首节点则不断尝试获取同步状态，获取成功则退出 CLH 同步队列。当线程执行完逻辑后，会释放同步状态，释放后会唤醒其后继节点。 共享模式 acquireShared()方法 源码 doc 翻译:以共享模式获取，忽略中断。 通过首先调用{@link #tryAcquireShared}来实现，成功返回。 否则线程排队，可能反复阻塞和解除阻塞，调用{@link #tryAcquireShared}直到成功。 简单点说就是这个方法会获取指定量的资源，获取成功则直接返回，获取失败则进入等待队列，直到获取到资源为止，整个过程忽略中断。 public final void acquireShared(int arg) { if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg); } tryAcquireShared()方法 tryAcquireShared()依然需要自定义实现类去实现。但是 AQS 已经把其返回值的语义定义好了：负值代表获取失败；0 代表获取成功，但没有剩余资源；正数表示获取成功，还有剩余资源，其他线程还可以去获取。 //共享式获取同步状态 protected int tryAcquireShared(int arg) { throw new UnsupportedOperationException(); } doAcquireShared()方法 源码 doc 翻译：以共享不间断模式获取 此方法用于将当前线程加入等待队列尾部休息，直到其他线程释放资源唤醒自己，自己成功拿到相应量的资源后才返回。 private void doAcquireShared(int arg) { //队列尾部添加共享模式的节点 final Node node = addWaiter(Node.SHARED); boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head) { //获取上一个节点,如果上一个节点时head，尝试获取资源 int r = tryAcquireShared(arg); if (r &gt;= 0) { setHeadAndPropagate(node, r);//成功有剩余资源，将head指向自己，唤醒之后的线程 p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; } } if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } setHeadAndPropagate()方法 设置队列头，并检查后继者是否在共享模式下等待，如果是传播，如果传播&gt; 0或PROPAGATE状态已设置。 这个方法除了重新标记 head 指向的节点外，还有一个重要的作用，那就是 propagate（传递）。 private void setHeadAndPropagate(Node node, int propagate) { Node h = head; // Record old head for check below setHead(node); /* * Try to signal next queued node if: * Propagation was indicated by caller, * or was recorded (as h.waitStatus either before * or after setHead) by a previous operation * (note: this uses sign-check of waitStatus because * PROPAGATE status may transition to SIGNAL.) * and * The next node is waiting in shared mode, * or we don't know, because it appears null * * The conservatism in both of these checks may cause * unnecessary wake-ups, but only when there are multiple * racing acquires/releases, so most need signals now or soon * anyway. */ if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) { Node s = node.next; if (s == null || s.isShared()) doReleaseShared(); } } doReleaseShared()方法 共享模式的释放操作 ，发出后续信号并确保传播。 （注意：对于独占模式，如果需要信号，只需调用数量来调用 head 的 unparkSuccessor） private void doReleaseShared() { /* * Ensure that a release propagates, even if there are other * in-progress acquires/releases. This proceeds in the usual * way of trying to unparkSuccessor of head if it needs * signal. But if it does not, status is set to PROPAGATE to * ensure that upon release, propagation continues. * Additionally, we must loop in case a new node is added * while we are doing this. Also, unlike other uses of * unparkSuccessor, we need to know if CAS to reset status * fails, if so rechecking. */ for (;;) { Node h = head; if (h != null &amp;&amp; h != tail) { int ws = h.waitStatus; if (ws == Node.SIGNAL) { if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); } else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS } if (h == head) // loop if head changed break; } } acquireShared总结： tryAcquireShared()尝试获取资源，成功则直接返回。 doAcquireShared()会将当前线程加入等待队列尾部休息，直到其他线程释放资源唤醒自己。它还会尝试着让唤醒传递到后面的节点。 releaseShared()方法 以共享模式发布。, 如果{@link #tryReleaseShared}返回 true，则通过解除阻塞一个或多个线程来实现。 public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false; } 4 java虚拟机 4.1 程序计数器 线程私有。程序计数器是一块较小的内存空间，他的作用可以看做是当前线程所执行的字节码的行号指示器，在虚拟机的概念模型里，字节码解释器工作时就是通过改变计数器的值来选择下一个需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖计数器。 java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要一个独立的程序计数器。 如果线程执行的是一个java方法，这个计数器记录的是正在执行的虚拟机字节码指令地址；如果正在执行的是native方法，则计数值为空（undefined） 4.2 java 虚拟机栈 线程私有，描述的是java方法执行的内存模型，生命周期与线程周期相同。每个方法执行时都会创建一个栈帧用于存储局部变量、操作栈、动态链接、方法出口等信息，每个方法被调用直至执行完成过程，对应一个栈帧在虚拟机栈中从入栈到出栈的过程。 4.3 本地方法栈 跟4.2类同 4.4 java堆 对大多数应用来说，java堆是java虚拟机所管理的内存中最大的一块。java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 是垃圾收集器管理的主要区域，因此很多时候也被称为“GC堆”，从内存 回收的角度看，现在收集器基本都是采用分代收集算法，所以java堆中还可以详细分为：新生代和老年代等 java堆傻可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，当前主流虚拟机都是按照可拓展来实现的，通过该-Xms初始化堆，-Xmx最大堆空间 4.5 方法区 是各个线程共享的内存区域，用于存储已经被虚拟机加载的类信息，常量，静态变量，即时编译后的代码等数据。 4.6 运行时常量池 方法区的一部分分。class文件中除了有类的版本，方法，字段，接口等描述信息外，还有一项是常量池，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。 运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性, Java语言并不要求常量一定只能在编译期产生, 也就是并非预置入Class文件中常量池的内容才能进入方法区运行时常量池, 运行期间也可能将新的常量放入翅中, 这种特性被开发人员利用的比较多的便是String类的intern() 方法.intern方法还是会先去查询常量池中是否有已经存在，如果存在，则返回常量池中的引用，这一点与之前没有区别，区别在于，如果在常量池找不到对应的字符串，则不会再将字符串拷贝到常量池，而只是在常量池中生成一个对原字符串的引用。 4.7 垃圾回收机制 4.7.1 对象回收算法 1. 引用计数算法。 核心思路为，给每个对象添加一个被引用计数器，被引用时+1，引用失效-1，等于0时表示该对象没有被引用，可以被回收。FlashPlayer/Python使用该算法，简单高效。但是，Java/C#并不采用该算法，因为该算法没有解决对象相互引用的问题，即：当两个对象相互引用且不被其它对象引用时，各自的引用计数为1，虽不为0，但仍然是可被回收的垃圾对象。 2. 可达分析法。 程序把所有的引用关系看作一张图，通过一系列的名为GC Roots的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链。当一个对象到 GC Roots 没有任何引用链相连（就是从 GC Roots 到这个对象不可达）时，则证明此对象是不可用的。 1会在cpu空闲的时候自动进行回收 2在堆内存存储满了之后 3主动调用System.gc()后尝试进行回收 4.7.2 垃圾回收算法 1. 标记清除算法 标记-清除（Mark-Sweep）算法可以算是最基础的垃圾收集算法，该算法主要分为“标记”和“清除”两个阶段。先标记可以被清除的对象，然后统一回收被标记要清除的对象，这个标记过程采用的就是可达性分析算法。 标记清除算法最大的缺点是在垃圾回收之后会产生大量的内存碎片，而如果内存碎片多了，当我们再创建一个占用内存比较大的对象时就没有足够的内存来分配，那么这个时候虚拟机就还要再次触发GC来清理内存后来给新的对象分配内存。 2. 复制算法 为了解决效率问题及标记清除算法的缺点，一种称为“复制”（Copying）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中一块，当这一块用完了，触发GC操作将存活的对象复制到另一个区域当中，然后再把使用过的内存空间一次清理掉。这样使得每次都对整个半区进行内存回收，内存分配也不用考虑内存碎片的问题，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。 这种算法最大的缺点是将原有内存分成了两块，每次只能使用区中一块，也就是损失了50%的内存空间，代价有点大。 3. 标记-整理算法 复制算法在对象存活率较高的情况下需要进行较多的复制操作，这样效率也会变低，更关键的是还需要浪费50%的内存空间，为了解决这些问题，于是“标记-整理”（Mark-Compact）算法就出来了，标记过程仍然使用可达性算法来判断，后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象向一端移动，然后直接清理边界以外的内存。标记-整理算法比较适合年老代的算法实现 4. 分代收集算法 当前商业虚拟机的垃圾回收都是采用“分代收集”（Generational Collection）算法，这种算法其实并没有什么新的思想，只是根据对象的存活周期的不同将内存划分为几块。一般把Java堆分为新生代和老年代，然后根据不同年代的特点采用适当的收集算法。 1、在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法。只需要付出少量存活对象的复制成本就可以完成收集。 2、老年代中因为对象存活率高、没有额外空间对他进行分配担保，就必须用标记-清除或者标记-整理。 4.7.3 垃圾收集器 1 Serial收集器 特性: 最基本、发展历史最久的收集器，采用复制算法的单线程收集器，**单线程一方面意味着它只会使用一个CPU或一条线程去完成垃圾收集工作，另一方面也意味着在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束为止，这个过程也称为 Stop The world。**后者意味着，在用户不可见的情况下要把用户正常工作的线程全部停掉，这显然对很多应用是难以接受的。 应用场景： Serial收集器依然是虚拟机运行在Client模式下的默认新生代收集器。 在用户的桌面应用场景中，可用内存一般不大（几十M至一两百M），可以在较短时间内完成垃圾收集（几十MS至一百多MS）,只要不频繁发生，这是可以接受的 优势： 简单而高效（与其他收集器的单线程相比），对于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。比如在用户的桌面应用场景中，可用内存一般不大（几十M至一两百M），可以在较短时间内完成垃圾收集（几十MS至一百多MS）,只要不频繁发生，这是可以接受的。 &quot;-XX:+UseSerialGC&quot;：添加该参数来显式的使用串行垃圾收集器； 2 ParNew收集器 特性: ParNew收集器其实就是Serial收集器的多线程版本，除了使用多条线程进行垃圾收集外，其余行为和Serial收集器完全一样，包括Serial收集器可用的所有控制参数、收集算法、Stop The world、对象分配规则、回收策略等都一样。在实现上也共用了相当多的代码。 应用场景： ParNew收集器是许多运行在Server模式下的虚拟机中首选的新生代收集器。很重要的原因是：除了Serial收集器之外，目前只有它能与CMS收集器配合工作（看图）。在JDK1.5时期，HotSpot推出了一款几乎可以认为具有划时代意义的垃圾收集器-----CMS收集器，这款收集器是HotSpot虚拟机中第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程同时工作。 优势： 在单CPU中的环境中，不会比Serail收集器有更好的效果，因为存在线程交互开销，甚至由于线程交互的开销，该收集器在两个CPU的环境中都不能百分百保证可以超越Serial收集器。当然，随着可用CPU数量的增加，它对于GC时系统资源的有效利用还是很有好处的，它默认开启的收集线程数与CPU数量相同。 设置参数：****&quot;-XX:+UseConcMarkSweepGC&quot;：指定使用CMS后，会默认使用ParNew作为新生代收集器；&quot;-XX:+UseParNewGC&quot;：强制指定使用ParNew； &quot;-XX:ParallelGCThreads&quot;：指定垃圾收集的线程数量，ParNew默认开启的收集线程与CPU的数量相同； 3 Parallel Scavenge 收集器 特性: Parallel Scavenge收集器是一个新生代收集器，它也是使用复制算法的收集器，也是并行的多线程收集器。 对比分析： Parallel Scavenge收集器 VS CMS等收集器： Parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量（Throughput）。 由于与吞吐量关系密切，Parallel Scavenge收集器也经常称为“吞吐量优先”收集器。 Parallel Scavenge收集器 VS ParNew收集器： Parallel Scavenge收集器与ParNew收集器的一个重要区别是它具有自适应调节策略。 应用场景： Parallel Scavenge收集器是虚拟机运行在Server模式下的默认垃圾收集器。 停顿时间短适合需要与用户交互的程序，良好的响应速度能提升用户体验；高吞吐量则可以高效率利用CPU时间，尽快完成运算任务，主要适合在后台运算而不需要太多交互的任务。 该收集器以高吞吐量为目标，就是减少垃圾收集时间，从而让用户代码获得更长的运行时间。所以适合那些运行在多个CPU上，并且专注于后台计算的应用程序，例如：执行批量处理任务、订单处理，工资支付，科学计算等。 设置参数： 虚拟机提供了-XX:MaxGCPauseMillis和-XX:GCTimeRatio两个参数来精确控制最大垃圾收集停顿时间和吞吐量大小。不过不要以为前者越小越好，GC停顿时间的缩短是以牺牲吞吐量和新生代空间换取的。 &quot;-XX:+MaxGCPauseMillis&quot;：控制最大垃圾收集停顿时间，大于0的毫秒数；这个参数设置的越小，停顿时间可能会缩短，但也会导致吞吐量下降，导致垃圾收集发生得更频繁。 ​ &quot;-XX:GCTimeRatio&quot;：设置垃圾收集时间占总时间的比率，0&lt;n&lt;100的整数，就相当于设置吞吐量的大小。垃圾收集执行时间占应用程序执行时间的比例的计算方法是： 1 / (1 + n)例如，选项-XX:GCTimeRatio=19，设置了垃圾收集时间占总时间的5%--1/(1+19)；默认值是1%--1/(1+99)，即n=99；垃圾收集所花费的时间是年轻一代和老年代收集的总时间； GC自适应的调节策略： Parallel Scavenge收集器有一个参数-XX:+UseAdaptiveSizePolicy。当这个参数打开之后，就不需要手工指定新生代的大小、Eden与Survivor区的比例、晋升老年代对象年龄等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量，这种调节方式称为GC自适应的调节策略（GC Ergonomics）。如果对于垃圾收集器运作原理不太了解，以至于在优化比较困难的时候，使用Parallel收集器配合自适应调节策略，把内存管理的调优任务交给虚拟机去完成将是一个不错的选择。 4 CMS收集器 CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。 目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常符合这类应用需求。 从名字（包含“Mark Sweep”）上就可以看出，CMS收集器是基于“标记-清除”算法实现的，它的运作过程相对于前面几种收集器来说更复杂一些，整个过程分为4个步骤，包括： 初始标记（CMS initial mark） 并发标记（CMS concurrent mark） 重新标记（CMS remark） 并发清除（CMS concurrent sweep） 其中，初始标记、重新标记这两个步骤仍然需要“Stop The World”。1.初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快；2.并发标记阶段就是进行GC Roots Tracing的过程；3.重新标记阶段是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。 由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。通过下图可以比较清除看见CMS收集器的运作步骤中并发和需要停顿的时间。 **优点：**并发收集、低停顿。 缺点： CMS收集器对CPU资源非常敏感。 其实，面向并发设计的程序都对CU资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。 CMS默认启动的回收线程数是（CPU数量+3）/ 4，也就是当CPU在4个以上时，并发回收时垃圾收集线程不少于25%的CPU资源，并且随着CPU数量的增加而下降。 但是当CPU不足4个（譬如2个）时，CMS对用户程序的影响就可能变得更大，如果本来CPU负载比较大，还分出一半的运算能力去执行收集器线程，就可能导致用户程序的执行速度忽然降低了50%，其实也让人无法接受。 为了应付这种情况，虚拟机提供了一种称为“增量式并发收集器”的CMS收集器变种，所做的事情和单CPU年代PC机操作系统使用抢占式来模拟多任务机制的思想一样，就是在并发标记、清理的时候让GC线程、用户线程交替运行，尽量减少GC线程的独占资源的时间，这样整个垃圾收集的过程会更长，但对用户程序的影响就会显得少一些，也就是速度下降没有那么明显。 实践证明，增量时的CMS收集器效果很一般，在目前版本中，i-CMS已经被声明为“deprecated”，即不再提倡用户使用。 CMS收集器无法处理浮动垃圾，可能出现“Concurrent Mode Failure”失败而导致另一次Full GC的产生。 由于CMS并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过一部分垃圾就称为“浮动垃圾”。 也是由于垃圾收集阶段用户线程还需要运行，那也就需要预留有足够的内存空间给用户线程使用，因此CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。 在JDK1.5的默认设置下，CMS收集器当老年代使用了68%的空间后就会被激活，这是一个偏保守的设置，如果在应用中老年代增长不是太快，可以适当调高参数-XX:CMSInitiatingOccupancyFraction的值来提高触发百分比，以便降低内存回收次数从而获取更好的性能。 在JDK1.6中，CMS收集器的启动阈值已经提升至92%。要是CMS运行期间预留的内存无法满足程序需要，就会出现一次“Concurrent Mode Failure”失败，这时虚拟机将启动后备预案：临时启动Serial Ol收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。所以说参数-XX:CMSInitiatingOccupancyFraction设置得太高很容易导致大量“Concurrent Mode Failure”失败，性能反而降低。 CMS是一款基于“标记-清除”算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。 空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次Full GC。 为了解决这个问题，CMS收集器提供了一个-XX:+UseCMSCompactAtFullCollection开关参数（默认是开启的），用于在CMS收集器顶不住要进行FullGC时开启内存碎片的合并整理过程，内存整理的过程是无法并发的，空间碎片问题没有了，但停顿时间不得不变长。 虚拟机设计者还提供了另外一个参数-XX:CMSFullGCsBeforeCompaction，这个参数是用于设置执行多少次不压缩的Full GC后，跟着来一次带压缩的（默认值为0，表示每次进入Full GC时都进行碎片整理）。 5 G1收集器 G1（Garbage-First），它是一款面向服务端应用的垃圾收集器，在多 CPU 和大内存的场景下有很好的性能。HotSpot 开发团队赋予它的使命是未来可以替换掉 CMS 收集器。 堆被分为新生代和老年代，其它收集器进行收集的范围都是整个新生代或者老年代，而 G1 可以直接对新生代和老年代一起回收。 G1 把堆划分成多个大小相等的独立区域（Region），新生代和老年代不再物理隔离。 通过引入 Region 的概念，从而将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。这种划分方法带来了很大的灵活性，使得可预测的停顿时间模型成为可能。通过记录每个 Region 垃圾回收时间以及回收所获得的空间（这两个值是通过过去回收的经验获得），并维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region，这也是 G1 (Garbage-First) 名称的来由。每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 如果不计算维护 Remembered Set 的操作，G1 收集器的运作大致可划分为以下几个步骤： 初始标记：仅仅只是标记一下GC Roots能直接关联的对象，并且修改TAMS(Next Top at Mark Start)的值，让下一阶段用户程序并发执行运行时，能在正确可用的Tegion中创建新对象，这阶段需要停顿线程，单耗时很短。 并发标记：从GC Root开始对堆中的对象进行可达性性分析，找出存活的对象，这阶段耗时较长，但可与用户程序吧并发执行。 最终标记：为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并行执行。 筛选回收：首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。 具备如下特点： 并行与并发：G1 能够利用多CPU、多核优势，缩短 STW（Stop The World）的时间，部分其它收集器原本需要停顿 Java 线程执行的 GC 操作，G1 收集器仍然可以通过并发的方式让 Java 程序继续执行。 分代收集： 分代概念在 G1 中仍然保留，虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但它仍然能够采取不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次 GC 的旧对象以获取更好的效果。 空间整合：整体来看是基于 标记 - 整理 算法实现的收集器，从局部（两个 Region 之间）上来看是基于 复制 算法实现的，这意味着运行期间不会产生内存空间碎片。 可预测的停顿：能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒。 4.7.4 内存分配与回收策略 1. Minor GC与Full GC Minor GC：指发生在新生代上的垃圾收集动作，因为新生代对象存活时间很短，因此 Minor GC 会频繁执行，执行的速度一般也会比较快。 Full GC（Major GC）：指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC（但非绝对，在Parallel Scavenge收集器的收集策略中就有直接进行Major GC的策略选择过程）。Major GC的速度一般会比Minor GC慢10倍以上。 2.内存分配策略 对象优先在Eden分配。大多数情况下，对象在新生代 Eden 区分配，当 Eden 区空间不够时，发起 Minor GC。 大对象直接进入老年代。大对象是指需要连续内存空间的对象，最典型的大对象是那种很长的字符串以及数组。经常出现大对象会提前触发垃圾收集以获取足够的连续空间分配给大对象。-XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 区和 Survivor 区之间的大量内存复制。 长期存活的对象进入老年代。为对象定义年龄计数器，对象在 Eden 出生并经过 Minor GC 依然存活，将移动到 Survivor 中，年龄就增加 1 岁，增加到一定年龄则移动到老年代中。-XX:MaxTenuringThreshold 用来定义年龄的阈值。 动态对象年龄判定。虚拟机并不是永远地要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代，如果在 Survivor 中相同年龄所有对象大小的总和大于 Survivor 空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代，无需等到 MaxTenuringThreshold 中要求的年龄。 空间分配担保。在发生 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的。如果不成立的话虚拟机会查看 HandlePromotionFailure 设置值是否允许担保失败，如果允许那么就会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC；如果小于，或者 HandlePromotionFailure 设置不允许冒险，那么就要进行一次 Full GC。 3.Full GC的触发条件 对于 Minor GC，其触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件： 调用System.gc( )。只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。 老年代空间不足。老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold调大对象进入老年代的年龄，让对象在新生代多存活一段时间。 空间分配担保失败。使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。具体内容请参考上面的第五小节。 JDK1.7及以前的永久代空间不足。在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静态变量等数据。当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS GC 的情况下也会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError。为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS GC。 Concurrent Mode Failure。执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足（可能是 GC 过程中浮动垃圾过多导致暂时性的空间不足），便会报 Concurrent Mode Failure 错误，并触发 Full GC。 5 设计模式 面向对象开发的六原则一法则: 单一原则:就一个类而言，应该只专注于做一件事和仅有一个引起它变化的原因，实现高内聚。 开放封闭:软件实体应当对扩展开放，对修改关闭。要做到开闭有两个要点：①抽象是关键，一个系统中如果没有抽象类或接口系统就没有扩展点；②封装可变性，将系统中的各种可变因素封装到一个继承结构中，如果多个可变因素混杂在一起，系统将变得复杂而换乱。 依赖倒置原则: 高层模块不应该依赖于低层模块，二者都应该依赖于抽象；抽象不应该依赖于细节，细节应该依赖于抽象。即面向接口编程。该原则说得直白和具体一些就是声明方法的参数类型、方法的返回类型、变量的引用类型时，尽可能使用抽象类型而不用具体类型，因为抽象类型可以被它的任何一个子类型所替代。 任何变量都不应该持有一个指向具体类的指针或者引用； 任何类都不应该从具体类派生； 任何方法都不应该覆写它的任何基类中的已经实现的方法。 里氏替换原则：子类对象必须能够替换掉所有父类对象。简单的说就是能用父类型的地方就一定能使用子类型，子类一定是增加父类的能力而不是减少父类的能力，因为子类比父类的能力更多，把能力多的对象当成能力少的对象来用当然没有任何问题。 接口隔离原则:接口要小而专，绝不能大而全.臃肿的接口是对接口的污染，既然接口表示能力，那么一个接口只应该描述一种能力，接口也应该是高度内聚的。 合成聚合复用原则:优先使用聚合或合成关系复用代码。 迪米特法则:迪米特法则又叫最少知识原则，一个对象应当对其他对象有尽可能少的了解。迪米特法则简单的说就是如何做到&quot;低耦合&quot;，门面模式和调停者模式就是对迪米特法则的践行。 介绍：中文译名：设计模式 - 可复用的面向对象软件元素） 中所提到的，总共有 23 种设计模式。这些模式可以分为三大类：创建型模式（Creational Patterns）、结构型模式（Structural Patterns）、行为型模式（Behavioral Patterns）。当然，我们还会讨论另一类设计模式：J2EE 设计模式。 5.1 单例模式 单例模式有以下特点：单例模式的定义，官方的定义总结起来就两句话，确保一个类只有一个实例（也就是类的对象），并且提供一个全局的访问点（外部通过这个访问点来访问该类的唯一实例）。 单例类只能有一个实例。 单例类必须自己创建自己的唯一实例，并给所有其他对象提供这一实例。 优点： 1、在内存里只有一个实例，减少了内存的开销，尤其是频繁的创建和销毁实例（比如管理学院首页页面缓存）。 ​ 2、避免对资源的多重占用（比如写文件操作）。 缺点： 没有接口，不能继承，与单一职责原则冲突，一个类应该只关心内部逻辑，而不关心外面怎么样来实例化。 使用场景： 1、要求生产唯一序列号。 2、WEB 中的计数器，不用每次刷新都在数据库里加一次，用单例先缓存起来。 3、创建的一个对象需要消耗的资源过多，比如 I/O 与数据库的连接等。 5.1.1 饿汉式实现 public class Singleton{ private static Singleton singleton=new Singleton();//主动创建 private Singleton(){}//私有构造方法 public static Singleton getInstance(){//提供全局的获取方法 return singleton; } } 5.1.2 懒汉实现 public class Singleton{ private static Singleton singleton;//先定义 private Singleton(){}//私有构造方法 public static Singleton getInstance(){ if(singleton==null){//需要用的时候才去创建 singleton=new Singleton(); } return singleton; } } 5.1.3懒汉式线程安全实现 public class Singleton{ private static Singleton singleton;//先定义 private Singleton(){}//私有构造方法 public static synchronized Singleton getInstance(){ if(singleton==null){ singleton=new Singleton(); } return singleton; } } public class Singleton{ private static Singleton singleton;//先定义 private Singleton(){}//私有构造方法 public static Singleton getInstance(){ synchronized（Singleton.class）{ if(singleton==null){ singleton=new Singleton(); } } return singleton; } } 5.1.4 双检锁/双重校验锁（DCL，即double-checked locking） public class Singleton{ private static Singleton singleton;//先定义 private Singleton(){}//私有构造方法 public static Singleton getInstance(){ if(singleton==null){//在线程安全的懒汉式上增加一个判空 synchronized（Singleton.class）{ if(singleton==null){ singleton=new Singleton(); } } } return singleton; } 5.1.5 登记式/静态内部类 这种方式能达到双检锁方式一样的功效，但实现更简单。对静态域使用延迟初始化，应使用这种方式而不是双检锁方式。这种方式只适用于静态域的情况，双检锁方式可在实例域需要延迟初始化时使用。 这种方式同样利用了 classloader 机制来保证初始化 instance 时只有一个线程，它跟第 3 种方式不同的是：第 3 种方式只要 Singleton 类被装载了，那么 instance 就会被实例化（没有达到 lazy loading 效果），而这种方式是 Singleton 类被装载了，instance 不一定被初始化。因为 SingletonHolder 类没有被主动使用，只有通过显式调用 getInstance 方法时，才会显式装载 SingletonHolder 类，从而实例化 instance。 public class Singleton{ private static class SingletonHolder{//创建内部类，提供唯一实例 private static final Singleton INSTANCE=new Singleton(); } private Singleton(){} public static final Singleton getInstance(){ return SingleonHolder.INSTANCE;//返回内部类唯一静态参数 } } 5.1.6 枚举 这种实现方式还没有被广泛采用，但这是实现单例模式的最佳方法。它更简洁，自动支持序列化机制，绝对防止多次实例化。 这种方式是 Effective Java 作者 Josh Bloch 提倡的方式，它不仅能避免多线程同步问题，而且还自动支持序列化机制，防止反序列化重新创建新的对象，绝对防止多次实例化。 public enum Singleton{ INSTANCE; public viod whateverMethod(){ } } 5.2 工厂模式 **意图：**定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行。 **主要解决：**主要解决接口选择的问题。 **何时使用：**我们明确地计划不同条件下创建不同实例时。 **如何解决：**让其子类实现工厂接口，返回的也是一个抽象的产品。 **关键代码：**创建过程在其子类执行。 //步骤一：创建一个接口 //Shape.java public interface Shap{ void draw(); } //步骤二：创建实现接口的实体类 //Rectangle.java public class Rectangle implements Shape{ @Override public void draw() { System.out.println(&quot;Inside Rectangle::draw() method.&quot;); } } //Square,java public class Square implements Shape{ @Override public void draw() { System.out.println(&quot;Inside Square::draw() method.&quot;); } } //Circle.java public class Circle implements Shape { @Override public void draw() { System.out.println(&quot;Inside Circle::draw() method.&quot;); } } //步骤三：创建一个工厂，生成基于给定信息的实体类的对象 public calss ShapeFactory{ public Shape getShape(String shapeType){ if(shapeType==null) return null; } if(shapeType.equalsIgnoreCase(&quot;CIRCLE&quot;)){ return new Circle(); } else if(shapeType.equalsIgnoreCase(&quot;RECTANGLE&quot;)){ return new Rectangle(); } else if(shapeType.equalsIgnoreCase(&quot;SQUARE&quot;)){ return new Square(); } return null; } //步骤四：使用工厂，通过传递类型信息来获取实体类的对象 public class FactoryPatternDemo { public static void main(String[] args) { ShapeFactory shapeFactory = new ShapeFactory(); //获取 Circle 的对象，并调用它的 draw 方法 Shape shape1 = shapeFactory.getShape(&quot;CIRCLE&quot;); //调用 Circle 的 draw 方法 shape1.draw(); Shape shape2 = shapeFactory.getShape(&quot;RECTANGLE&quot;); shape2.draw(); Shape shape3 = shapeFactory.getShape(&quot;SQUARE&quot;); shape3.draw(); } } 5.3 抽象工厂模式 抽象工厂模式（Abstract Factory Pattern）是围绕一个超级工厂创建其他工厂。该超级工厂又称为其他工厂的工厂。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 在抽象工厂模式中，接口是负责创建一个相关对象的工厂，不需要显式指定它们的类。每个生成的工厂都能按照工厂模式提供对象。 **意图：**提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。 **主要解决：**主要解决接口选择的问题。 **何时使用：**系统的产品有多于一个的产品族，而系统只消费其中某一族的产品。 **如何解决：**在一个产品族里面，定义多个产品。 **关键代码：**在一个工厂里聚合多个同类产品。 **优点：**当一个产品族中的多个对象被设计成一起工作时，它能保证客户端始终只使用同一个产品族中的对象。 **缺点：**产品族扩展非常困难，要增加一个系列的某一产品，既要在抽象的 Creator 里加代码，又要在具体的里面加代码。 使用场景： 1、QQ 换皮肤，一整套一起换。 2、生成不同操作系统的程序。 **注意事项：**产品族难扩展，产品等级易扩展。 //步骤一：为形状创建一个接口 //Shap.java public interface Shap{ void draw(); } //步骤二：创建实现接口的实体类 //Rectangle.java public class Rectangle implements Shape{ @Override public void draw() { System.out.println(&quot;Inside Rectangle::draw() method.&quot;); } } //Square,java public class Square implements Shape{ @Override public void draw() { System.out.println(&quot;Inside Square::draw() method.&quot;); } } //Circle.java public class Circle implements Shape { @Override public void draw() { System.out.println(&quot;Inside Circle::draw() method.&quot;); } } //步骤三：为颜色创建一个接口 //Color.java public interface Color{ void fill(); } //步骤四：创建实现接口的实体类 //Red.java public class Red implements Color { @Override public void fill() { System.out.println(&quot;Inside Red::fill() method.&quot;); } } //Green.java public class Green implements Color { @Override public void fill() { System.out.println(&quot;Inside Green::fill() method.&quot;); } } //Blue.java public class Blue implements Color { @Override public void fill() { System.out.println(&quot;Inside Blue::fill() method.&quot;); } } //步骤五：为Color和Shape对象创建抽象类来获取工厂 //AbstractFactory.java public abstract class AbstractFactory { public abstract Color getColor(String color); public abstract Shape getShape(String shape); } //步骤六：创建扩展了 AbstractFactory 的工厂类，基于给定的信息生成实体类的对象。 //ShapeFactory.java public class ShapeFactory extends AbstractFactory { @Override public Shape getShape(String shapeType){ if(shapeType == null){ return null; } if(shapeType.equalsIgnoreCase(&quot;CIRCLE&quot;)){ return new Circle(); } else if(shapeType.equalsIgnoreCase(&quot;RECTANGLE&quot;)){ return new Rectangle(); } else if(shapeType.equalsIgnoreCase(&quot;SQUARE&quot;)){ return new Square(); } return null; } @Override public Color getColor(String color) { return null; } } //ColorFactory.java public class ColorFactory extends AbstractFactory { @Override public Shape getShape(String shapeType){ return null; } @Override public Color getColor(String color) { if(color == null){ return null; } if(color.equalsIgnoreCase(&quot;RED&quot;)){ return new Red(); } else if(color.equalsIgnoreCase(&quot;GREEN&quot;)){ return new Green(); } else if(color.equalsIgnoreCase(&quot;BLUE&quot;)){ return new Blue(); } return null; } } //步骤七：创建一个工厂创造器/生成器类，通过传递形状或颜色信息来获取工厂 //FactoryProducer.java public class FactoryProducer { public static AbstractFactory getFactory(String choice){ if(choice.equalsIgnoreCase(&quot;SHAPE&quot;)){ return new ShapeFactory(); } else if(choice.equalsIgnoreCase(&quot;COLOR&quot;)){ return new ColorFactory(); } return null; } } //步骤八：使用 FactoryProducer 来获取 AbstractFactory，通过传递类型信息来获取实体类的对象。 //AbstractFactoryPatternDemo.java public class AbstractFactoryPatternDemo { public static void main(String[] args) { //获取形状工厂 AbstractFactory shapeFactory = FactoryProducer.getFactory(&quot;SHAPE&quot;); //获取形状为 Circle 的对象 Shape shape1 = shapeFactory.getShape(&quot;CIRCLE&quot;); //调用 Circle 的 draw 方法 shape1.draw(); //获取形状为 Rectangle 的对象 Shape shape2 = shapeFactory.getShape(&quot;RECTANGLE&quot;); //调用 Rectangle 的 draw 方法 shape2.draw(); //获取形状为 Square 的对象 Shape shape3 = shapeFactory.getShape(&quot;SQUARE&quot;); //调用 Square 的 draw 方法 shape3.draw(); //获取颜色工厂 AbstractFactory colorFactory = FactoryProducer.getFactory(&quot;COLOR&quot;); //获取颜色为 Red 的对象 Color color1 = colorFactory.getColor(&quot;RED&quot;); //调用 Red 的 fill 方法 color1.fill(); //获取颜色为 Green 的对象 Color color2 = colorFactory.getColor(&quot;Green&quot;); //调用 Green 的 fill 方法 color2.fill(); //获取颜色为 Blue 的对象 Color color3 = colorFactory.getColor(&quot;BLUE&quot;); //调用 Blue 的 fill 方法 color3.fill(); } } 5.4 建造者模式 建造者模式（Builder Pattern）使用多个简单的对象一步一步构建成一个复杂的对象。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。一个 Builder 类会一步一步构造最终的对象。该 Builder 类是独立于其他对象的。 **意图：**将一个复杂的构建与其表示相分离，使得同样的构建过程可以创建不同的表示。 **主要解决：**主要解决在软件系统中，有时候面临着&quot;一个复杂对象&quot;的创建工作，其通常由各个部分的子对象用一定的算法构成；由于需求的变化，这个复杂对象的各个部分经常面临着剧烈的变化，但是将它们组合在一起的算法却相对稳定。 **何时使用：**一些基本部件不会变，而其组合经常变化的时候。 **如何解决：**将变与不变分离开。 **关键代码：**建造者：创建和提供实例，导演：管理建造出来的实例的依赖关系。 应用实例： 1、去肯德基，汉堡、可乐、薯条、炸鸡翅等是不变的，而其组合是经常变化的，生成出所谓的&quot;套餐&quot;。 2、JAVA 中的 StringBuilder。 优点： 1、建造者独立，易扩展。 2、便于控制细节风险。 缺点： 1、产品必须有共同点，范围有限制。 2、如内部变化复杂，会有很多的建造类。 使用场景： 1、需要生成的对象具有复杂的内部结构。 2、需要生成的对象内部属性本身相互依赖。 **注意事项：**与工厂模式的区别是：建造者模式更加关注与零件装配的顺序。 5.5 原型模式 ","link":"https://kily-007.github.io/post/笔记-java基础/"},{"title":"笔记-计算机网络","content":"1 七层网络模型 2 TCP(传输层)与UDP协议 UDP 在传送数据之前不需要先建立连接，远地主机在收到 UDP 报文后，不需要给出任何确认。虽然 UDP 不提供可 靠交付，但在某些情况下 UDP 确是一种有效的工作方式（一般用于即时通信），比如： QQ 语 音、 QQ 视频 、直 播等等。 TCP 提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。 TCP 不提供广播或多播 服务。由于 TCP 要提供可靠的，面向连接的运输服务（TCP的可靠体现在TCP在传递数据之前，会有三次握手来建立 连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资 源），这一难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增 大很多，还要占用许多处理机资源。TCP 一般用于文件传输、发送和接收邮件、远程登录等场景。 3 浏览器中输入url地址显示主页的全过程 1 URL RL（Uniform Resource Locator），统一资源定位符，用于定位互联网上资源，俗称网址。 语法：scheme://host.domain:port/path/filename 例子： https:// www.baidu.com:80 2 域名解析 域名解析：DNS 协议提供通过域名查找 IP 地址，或逆向从 IP 地址反查域名的服务。DNS 是一个网络服务器，我们的域名解析简单来说就是在 DNS 上记录一条信息记录。 目的： 浏览器是不能通过url地址查找到对应的服务器，需通过ip地址。 通过域名查找IP地址的过程： 浏览器缓存 操作系统缓存 路由缓存 ISP 的 DNS 服务器：ISP 是互联网服务提供商(Internet Service Provider)的简称，ISP 有专门的 DNS 服务器应对 DNS 查询请求。 根服务器 例如，要查询www.baidu.com的IP地址（DNS解析url）： 浏览器搜索自己的DNS缓存（维护一张域名与IP地址的对应表） 若没有，则搜索操作系统中的DNS缓存（维护一张域名与IP地址的对应表） 若没有，则搜索操作系统的hosts文件（Windows环境下，维护一张域名与IP地址的对应表，位置一般在 C:\\Windows\\System32\\drivers\\etc\\hosts） 若没有，则操作系统将域名发送至 本地域名服务器- -（递归查询方式），本地域名服务器 查询自己的DNS缓存，查找成功则返回结果，否则，（以下是迭代查询方式） 本地域名服务器 向根域名服务器（其虽然没有每个域名的具体信息，但存储了负责每个域，如com、net、org等的解析的顶级域名服务器的地址）发起请求，此处，根域名服务器返回com域的顶级域名服务器的地址 本地域名服务器 向com域的顶级域名服务器发起请求，返回baidu.com权限域名服务器（权限域名服务器，用来保存该区中的所有主机域名到IP地址的映射）地址 本地域名服务器 向baidu.com权限域名服务器发起请求，得到www.baidu.com的IP地址本地域名服务器 将得到的IP地址返回给操作系统，同时自己也将IP地址缓存起来 操作系统将 IP 地址返回给浏览器，同时自己也将IP地址缓存起来 至此，浏览器已经得到了域名对应的IP地址 3 TCP连接（三次握手） 客户端–发送带有 SYN 标志的数据包–一次握手–服务端 服务端–发送带有 SYN/ACK 标志的数据包–二次握手–客户端 客户端–发送带有带有 ACK 标志的数据包–三次握手–服务端 简单来说， 客户端：兄die，听得到吗？ 服务器：我收到，你收到吗？ 客户端：我也收到。 为什么要传回 SYN ？ ·接收端传回发送端所发送的 SYN 是为了告诉发送端，我接收到的信息确实就是你所发送的信号了。 SYN 是 TCP/IP 建立连接时使用的握手信号。在客户机和服务器之间建立正常的 TCP 网络连接时，客户机首先 发出一个 SYN 消息，服务器使用 SYN-ACK 应答表示接收到了这个消息，后客户机再以 ACK(Acknowledgement[汉译：确认字符 ,在数据通信传输中，接收站发给发送站的一种传输控制字符。它表 示确认发来的数据已经接受无误。 ]）消息响应。这样在客户机和服务器之间才能建立起可靠的TCP连接，数据 才可以在客户机和服务器之间传递。 传了 SYN,为啥还要传 ACK ？ 双方通信无误必须是两者互相发送信息都无误。传了 SYN，证明发送方到接收方的通道没有问题，但是接收方到发送 方的通道还需要 ACK 信号来进行验证。 4 发送http请求 客户端发送一个HTTP请求到服务器的请求消息包括以下格式：请求行（request line）、请求头部（header）、空行和请求数据四个部分组成 请求方法：包含 8 种：GET、POST、PUT、DELETE、PATCH、HEAD、OPTIONS、TRACE 序号 方法 描述 1 GET 请求指定的页面信息，并返回实体主体。 2 HEAD 类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头 3 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。 4 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 5 DELETE 请求服务器删除指定的页面。 6 CONNECT HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。 7 OPTIONS 允许客户端查看服务器的性能。 8 TRACE 回显服务器收到的请求，主要用于测试或诊断。 9 PATCH 是对 PUT 方法的补充，用来对已知资源进行局部更新 。 5 处理请求并返回http报文 http响应报文 包含： 响应行、响应头、响应主体。 HTTP状态码由三个十进制数字组成，第一个十进制数字定义了状态码的类型，后两个数字没有分类的作用。HTTP状态码共分为5种类型： 分类 分类描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 成功，操作被成功接收并处理 3** 重定向，需要进一步的操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程中发生了错误 实例： 下面实例是一点典型的使用GET来传递数据的实例： 客户端请求： GET /hello.txt HTTP/1.1 User-Agent: curl/7.16.3 libcurl/7.16.3 OpenSSL/0.9.7l zlib/1.2.3 Host: www.example.com Accept-Language: en, mi 服务端响应: HTTP/1.1 200 OK Date: Mon, 27 Jul 2009 12:28:53 GMT Server: Apache Last-Modified: Wed, 22 Jul 2009 19:15:56 GMT ETag: &quot;34aa387-d-1568eb00&quot; Accept-Ranges: bytes Content-Length: 51 Vary: Accept-Encoding Content-Type: text/plain 输出结果： Hello World! My payload includes a trailing CRLF. 6 TCP释放连接（四次挥手） 浏览器所在主机向服务器发出连接释放报文，然后停止发送数据； 服务器接收到释放报文后发出确认报文，然后将服务器上未传送完的数据发送完； 服务器数据传输完毕后，向客户机发送连接释放报文； 客户机接收到报文后，发出确认，然后等待一段时间后，释放TCP连接； 即为四次挥手过程： 断开一个 TCP 连接则需要“四次挥手”： 客户端-发送一个 FIN，用来关闭客户端到服务器的数据传送 服务器-收到这个 FIN，它发回一 个 ACK，确认序号为收到的序号加1 。和 SYN 一样，一个 FIN 将占用一个序号 服务器-关闭与客户端的连接，发送一个FIN给客户端 客户端-发回 ACK 报文确认，并将确认序号设置为收到序号加1 简单来说： 客户端：我说完了； 服务端：我知道了，继续把我要说的说完； 服务端：说完后，通知客户端我要结束了； 客户端：收到！！ 4 http(应用层协议)长连接与短连接 4.1 http短连接 在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中 断连接。当客户端浏览器访问的某个HTML或其他类型的Web页中包含有其他的Web资源（如JavaScript文件、图像 文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个HTTP会话。 4.2 http长连接 HTTP的长连接和短连接本质上是TCP长连接和短连接。 而从HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头加入这行代码： 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客 户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时 间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 4.3 http与TCP/IP协议的关系 HTTP属于应用层协议，在传输层使用TCP协议，在网络层使用IP协议。 IP协议主要解决网络路由和寻址问题，TCP协议主要解决如何在IP层之上可靠地传递数据包，使得网络上接收端收到发送端所发出的所有包，并且顺序与发送顺序一致。TCP协议是可靠的、面向连接的。 4.4 如何理解http是无 状态的 HTTP协议是无状态的，指的是协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。也就是说，打开一个服务器上的网页和上一次打开这个服务器上的网页之间没有任何联系。HTTP是一个无状态的面向连接的协议，无状态不代表HTTP不能保持TCP连接，更不能代表HTTP使用的是UDP协议（无连接）。 4.5 TCP短连接 模拟一下TCP短连接的情况：client向server发起连接请求，server接到请求，然后双方建立连接（三次握手）。client向server发送消息，server回应client，然后一次请求就完成了。这时候双方任意都可以发起close操作，不过一般都是client先发起close操作。上述可知，短连接一般只会在 client/server间传递一次请求操作。 短连接的优点是：管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段。 4.6 TCP长连接 我们再模拟一下长连接的情况：client向server发起连接，server接受client连接，双方建立连接（三次握手）。client与server完成一次请求后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。TCP的保活功能主要为服务器应用提供。如果客户端已经消失而连接未断开，则会使得服务器上保留一个半开放的连接，而服务器又在等待来自客户端的数据，此时服务器将永远等待客户端的数据。保活功能就是试图在服务端器端检测到这种半开放的连接。 如果一个给定的连接在两小时内没有任何动作，服务器就向客户发送一个探测报文段，根据客户端主机响应探测4个客户端状态： 客户主机依然正常运行，且服务器可达。此时客户的TCP响应正常，服务器将保活定时器复位。 客户主机已经崩溃，并且关闭或者正在重新启动。上述情况下客户端都不能响应TCP。服务端将无法收到客户端对探测的响应。服务器总共发送10个这样的探测，每个间隔75秒。若服务器没有收到任何一个响应，它就认为客户端已经关闭并终止连接。 客户端崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。 客户机正常运行，但是服务器不可达。这种情况与第二种状态类似。 4.7 长连接与短连接的优缺点 长连接可以省去较多的TCP建立和关闭的操作，减少浪费，节约时间。对于频繁请求资源的客户端适合使用长连接。在长连接的应用场景下，client端一般不会主动关闭连接，当client与server之间的连接一直不关闭，随着客户端连接越来越多，server会保持过多连接。这时候server端需要采取一些策略，如关闭一些长时间没有请求发生的连接，这样可以避免一些恶意连接导致server端服务受损；如果条件允许则可以限制每个客户端的最大长连接数，这样可以完全避免恶意的客户端拖垮整体后端服务。 短连接对于服务器来说管理较为简单，存在的连接都是有用的连接，不需要额外的控制手段。但如果客户请求频繁，将在TCP的建立和关闭操作上浪费较多时间和带宽。 长连接和短连接的产生在于client和server采取的关闭策略。不同的应用场景适合采用不同的策略。 4.8 适用场景 **长连接：**多用于操作频繁，点对点的通讯，而且连接数不能太多情况。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。 **短链接：**而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。 ","link":"https://kily-007.github.io/post/笔记-计算机网络/"},{"title":"剑指offer-字符串转整数","content":" 将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。 public int StrToInt(String str) { if(str.equals(&quot;&quot;)) return 0; if(str==&quot;-2147483648&quot;) return -2147483648; if(str.charAt(0)=='+'||str.charAt(0)=='-') { for (int i = 1; i &lt; str.length(); i++) { if(str.charAt(i)&gt;'9'||str.charAt(i)&lt;'0') return 0; } int rs=0; for (int i = 0; i &lt; str.length()-1; i++) { rs+=(str.charAt(i+1)-48)*(Math.pow(10, str.length()-(i+2))); } if(str.charAt(0)=='-') { return -rs; } return rs; }else if(str.charAt(0)&lt;='9'&amp;&amp;str.charAt(0)&gt;='0'){//第一个字符为数字 for (int i = 1; i &lt; str.length(); i++) { if(str.charAt(i)&gt;'9'||str.charAt(i)&lt;'0') return 0; } int rs=0; for (int i = 0; i &lt; str.length(); i++) { rs+=(str.charAt(i)-48)*(Math.pow(10, str.length()-(i+1))); } return rs; }else {//第一个字符为其他 return 0; } } ","link":"https://kily-007.github.io/post/剑指offer-字符串转整数/"},{"title":"剑指offer-除法运算","content":"给定一个数组A[0,1,...,n-1],请构建一个数组B[0,1,...,n-1],其中B中的元素B[i]=A[0]A[1]...*A[i-1]A[i+1]...*A[n-1]。不能使用除法。 public int[] multiply(int[] A) { if (A.length != 0) { int[] B = new int[A.length]; for (int i = 0; i &lt; B.length; i++) { B[i]=1; } for (int i = 0; i &lt; B.length; i++) { for (int j = 0; j &lt; A.length; j++) { if(j!=i) B[i]*=A[j]; } } return B; }else { return new int [] {}; } } ","link":"https://kily-007.github.io/post/剑指offer-除法运算/"},{"title":"Hello Gridea","content":"👏 欢迎使用 Gridea ！ ✍️ Gridea 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ... Github Gridea 主页 示例网站 特性👇 📝 你可以使用最酷的 Markdown 语法，进行快速创作 🤡 🌉 你可以给文章配上精美的封面图和在文章任意位置插入图片 🏷️ 你可以对文章进行标签分组 📋 你可以自定义菜单，甚至可以创建外部链接菜单 💻 你可以在 Windows，MacOS 或 Linux 设备上使用此客户端 🌎 你可以使用 𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌 或 Coding Pages 向世界展示，未来将支持更多平台 💬 你可以进行简单的配置，接入 Gitalk 或 DisqusJS 评论系统 🇬🇧 你可以使用中文简体或英语 🌁 你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力 🖥 你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步 🌱 当然 Gridea 还很年轻，有很多不足，但请相信，它会不停向前 🏃 未来，它一定会成为你离不开的伙伴 尽情发挥你的才华吧！ 😘 Enjoy~ ","link":"https://kily-007.github.io/post/hello-gridea/"}]}